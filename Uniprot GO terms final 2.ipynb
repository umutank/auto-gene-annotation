{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee5227fb-213d-4807-a457-ee4542b27929",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-29 11:54:17 [INFO    ] --- Script Execution Started ---\n",
      "2025-04-29 11:54:17 [INFO    ] Input file: /Users/umuttank/Downloads/Results.xlsx\n",
      "2025-04-29 11:54:17 [INFO    ] Output file: /Users/umuttank/Downloads/Results.xlsx\n",
      "2025-04-29 11:54:17 [INFO    ] Pfam Clan file: Pfam-A.clans.tsv.gz\n",
      "2025-04-29 11:54:17 [INFO    ] Log file: annotation_log_20250429.log\n",
      "2025-04-29 11:54:17 [INFO    ] Requests session configured with max 5 retries and backoff factor 0.4.\n",
      "2025-04-29 11:54:17 [INFO    ] Successfully loaded Pfam clan data (24424 rows) from Pfam-A.clans.tsv.gz\n",
      "2025-04-29 11:54:17 [INFO    ] Initialized global caches for API results.\n",
      "2025-04-29 11:54:17 [INFO    ] g:Profiler client initialized.\n",
      "2025-04-29 11:54:17 [INFO    ] --- Starting Main Processing ---\n",
      "2025-04-29 11:54:17 [INFO    ] Loaded input Excel: /Users/umuttank/Downloads/ladin test.xlsx with sheets: ['Sheet1']\n",
      "2025-04-29 11:54:17 [INFO    ] Initialized Excel writer for /Users/umuttank/Downloads/ladin test_Annotated_20250429.xlsx\n",
      "2025-04-29 11:54:17 [WARNING ] Specified EV sheet 'EV vs EV_ISD' not found in the input Excel file. EV Control Background flag cannot be applied.\n",
      "2025-04-29 11:54:17 [INFO    ] --- Processing sheet 1/1: 'Sheet1' ---\n",
      "2025-04-29 11:54:17 [INFO    ] Loaded sheet 'Sheet1' (5 rows x 1 columns)\n",
      "2025-04-29 11:54:17 [INFO    ] 'Significant' column not found in sheet 'Sheet1'. Processing all rows with entries in 'Gene names'.\n",
      "2025-04-29 11:54:17 [INFO    ] Selected 5 rows with gene names to process.\n",
      "2025-04-29 11:54:17 [INFO    ] Annotating 5 unique valid gene symbols for 'Sheet1'...\n",
      "2025-04-29 11:54:29 [INFO    ] Finished fetching annotations for 'Sheet1'.                          \n",
      "2025-04-29 11:54:29 [INFO    ] Dynamic UniProt Keyword columns found for this sheet: ['UniProt KW: Biological_process', 'UniProt KW: Cellular_component', 'UniProt KW: Coding_sequence_diversity', 'UniProt KW: Disease', 'UniProt KW: Domain', 'UniProt KW: Ligand', 'UniProt KW: Molecular_function', 'UniProt KW: PTM', 'UniProt KW: Technical_term']\n",
      "2025-04-29 11:54:29 [INFO    ] Mapping 36 annotation columns back to the sheet...\n",
      "2025-04-29 11:54:29 [INFO    ] Applying flags and QC checks...\n",
      "2025-04-29 11:54:29 [WARNING ] Skipping EV LFQ Background flag in sheet 'Sheet1': Required columns missing (e.g., ['EV_01', 'EV_02', 'EV_03', 'EV_04'], log2FC).\n",
      "2025-04-29 11:54:29 [WARNING ] Skipping Name Match QC in sheet 'Sheet1': Columns 'Protein names' or 'Protein Description' missing.\n",
      "2025-04-29 11:54:29 [INFO    ] Reordering columns for final output...\n",
      "2025-04-29 11:54:29 [INFO    ] Successfully reordered columns. Final shape for sheet 'Sheet1': (5x42)\n",
      "2025-04-29 11:54:29 [INFO    ] Writing sheet 'Sheet1' (5 rows, 42 cols).\n",
      "2025-04-29 11:54:29 [INFO    ] Successfully wrote and saved sheet 'Sheet1' to /Users/umuttank/Downloads/ladin test_Annotated_20250429.xlsx.\n",
      "2025-04-29 11:54:29 [INFO    ] --- Main Processing Finished ---\n",
      "2025-04-29 11:54:29 [INFO    ] âœ… Annotation and saving complete.\n",
      "2025-04-29 11:54:29 [INFO    ]    Processed/Written Sheets: 1/1\n",
      "2025-04-29 11:54:29 [INFO    ]    Output saved at: /Users/umuttank/Downloads/ladin test_Annotated_20250429.xlsx\n",
      "2025-04-29 11:54:29 [INFO    ]    Log file saved at: annotation_log_20250429.log\n",
      "2025-04-29 11:54:29 [INFO    ]    Unique dynamic UniProt Keyword columns generated across all sheets: ['UniProt KW: Biological_process', 'UniProt KW: Cellular_component', 'UniProt KW: Coding_sequence_diversity', 'UniProt KW: Disease', 'UniProt KW: Domain', 'UniProt KW: Ligand', 'UniProt KW: Molecular_function', 'UniProt KW: PTM', 'UniProt KW: Technical_term']\n",
      "2025-04-29 11:54:29 [INFO    ] --- Script Execution Finished ---\n",
      "2025-04-29 11:54:29 [INFO    ] Total script execution time: 12.25 seconds.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# SCRIPT HEADER\n",
    "# ==============================================================================\n",
    "#\n",
    "# Author:       [Umut TANK]\n",
    "# Date Created: [April 2025]\n",
    "# Contact:      [umuttank@hotmail.com]\n",
    "# Project:      [Annotation of Proteomics Results for my Project]\n",
    "#\n",
    "# ==============================================================================\n",
    "# SCRIPT DESCRIPTION\n",
    "# ==============================================================================\n",
    "#\n",
    "# **Overall Purpose:**\n",
    "# This script automates the process of fetching and integrating biological annotations\n",
    "# for lists of proteins/genes from experimental results (e.g., proteomics analysis).\n",
    "# It reads data from specified sheets within an input Excel file. It checks for a\n",
    "# 'Significant' column; if present, it filters for significant entries ('+'), otherwise\n",
    "# it processes all rows containing gene names. It retrieves functional information\n",
    "# from various online databases, flags potential background contaminants, and\n",
    "# outputs a new, richly annotated Excel file.\n",
    "#\n",
    "# **Workflow:**\n",
    "# 1.  **Configuration:** Reads settings for file paths, API parameters (retries,\n",
    "#     timeouts), analysis thresholds, background keywords, and specific column names.\n",
    "# 2.  **Input Loading:** Loads the specified input Excel file (`.xlsx`).\n",
    "# 3.  **Pfam Data Loading:** Loads Pfam domain-to-clan mapping information from a local TSV file.\n",
    "# 4.  **Sheet Iteration:** Processes each sheet within the input Excel file sequentially.\n",
    "# 5.  **Row Selection (Conditional):**\n",
    "#     *   Checks if a 'Significant' column exists.\n",
    "#     *   If YES: Filters rows marked as significant ('+') for annotation.\n",
    "#     *   If NO: Selects all rows containing a non-empty 'Gene names' entry for annotation.\n",
    "# 6.  **Gene Symbol Extraction:** Extracts the primary gene symbol from the relevant column\n",
    "#     in the selected rows (handling potentially semicolon-separated lists).\n",
    "# 7.  **Annotation Fetching (for Unique Selected Genes):** For each unique gene symbol:\n",
    "#     *   Uses a caching mechanism to avoid redundant API calls for the same gene.\n",
    "#     *   Fetches data from UniProt (protein description, function, location, keywords,\n",
    "#       cross-references including Pfam, etc. **Note: UniProt GO terms are NOT fetched directly**).\n",
    "#     *   Fetches interacting partners from STRING DB (physical and functional).\n",
    "#     *   Fetches functional enrichment terms from g:Profiler (GO, pathways, complexes).\n",
    "#     *   Fetches detailed GO terms (BP, MF, CC) from QuickGO.\n",
    "#     *   Applies configured retries and backoff delays for robustness against API issues.\n",
    "# 8.  **Data Processing & Integration:**\n",
    "#     *   Parses the fetched data (e.g., extracts specific fields, looks up Pfam descriptions/clans).\n",
    "#     *   Handles dynamic UniProt keyword categorization.\n",
    "# 9.  **Mapping Annotations:** Maps the retrieved and processed annotations back to the\n",
    "#     corresponding selected rows in the sheet's DataFrame.\n",
    "# 10. **Background Flagging:** Applies flags based on:\n",
    "#     *   Keywords in protein descriptions ('Sticky Binder Flag').\n",
    "#     *   Expression patterns in EV control columns ('EV LFQ Background Flag').\n",
    "#     *   Significance in a separate control sheet ('EV Control Background Flag').\n",
    "# 11. **QC Check:** Compares input protein names to UniProt descriptions ('Name Match Discrepancy').\n",
    "# 12. **Column Reordering:** Arranges the columns into a logical order, inserting annotations\n",
    "#     after the 'Gene names' column.\n",
    "# 13. **Output Writing:** Writes the annotated DataFrame for the current sheet to the\n",
    "#     output Excel file using a safe writing function (handles sheet name cleaning).\n",
    "#     *Note: Currently configured to save the entire workbook after each sheet is written.*\n",
    "# 14. **Logging:** Records progress, warnings, errors, configuration details, and timing\n",
    "#     to both the console and a timestamped log file.\n",
    "#\n",
    "# **Input:**\n",
    "# *   **Primary:** An Excel file (`.xlsx`) containing one or more sheets with protein/gene data.\n",
    "#     Expected columns: \"Gene names\". Optionally: \"Significant\", \"Protein names\",\n",
    "#     \"log2FC\", and EV intensity columns (e.g., \"EV_01\").\n",
    "# *   **Secondary (Optional but Recommended):** A local Pfam Clan TSV file (e.g., `Pfam-A.clans.tsv.gz`)\n",
    "#     for mapping Pfam domains to descriptions and clans.\n",
    "#\n",
    "# **Output:**\n",
    "# *   **Primary:** A new Excel file (`.xlsx`) named based on the input file plus \"_Annotated_YYYYMMDD.xlsx\",\n",
    "#     containing the annotated data for the selected rows from each processed sheet.\n",
    "# *   **Secondary:** A log file (`.log`) named `annotation_log_YYYYMMDD.log` containing detailed\n",
    "#     information about the script's execution.\n",
    "#\n",
    "# **Key Features & Annotations Added:**\n",
    "# *   **UniProt:** ID, Description, Function, Subcellular Location, Molecular Mass (kDa),\n",
    "#     Alternative Names, Catalytic Activity, Associated Diseases, Sequence Similarities,\n",
    "#     Pathways, Protein Existence Level, Categorized Keywords.\n",
    "# *   **Pfam (via UniProt & Lookup):** PFAM Domain IDs, PFAM Descriptions, PFAM Clan Names.\n",
    "# *   **STRING DB:** High-confidence Physical Interactors, Functional Partners.\n",
    "# *   **g:Profiler:** Functional enrichment terms (GO:BP, GO:MF, GO:CC, REAC, KEGG, WP, CORUM).\n",
    "# *   **QuickGO:** Detailed GO terms (BP, MF, CC) fetched via QuickGO API.\n",
    "# *   **Flags:** Sticky Binder, EV LFQ Background, EV Control Background, Name Match QC.\n",
    "# *   **Utilities:** API request caching, robust API retries with backoff, detailed logging,\n",
    "#     conditional row selection based on 'Significant' column presence.\n",
    "#\n",
    "# **Configuration:**\n",
    "# Key parameters like input/output paths, API settings, background keywords, column names,\n",
    "# and analysis thresholds can be adjusted in the 'CONFIGURATION SETTINGS' (Section 3)\n",
    "# and 'COLUMN NAME CONSTANTS' (Section 4) sections of the script.\n",
    "#\n",
    "# ==============================================================================\n",
    "# (Script Code Starts Below)\n",
    "# ==============================================================================\n",
    "\n",
    "# === 1. INSTALL LIBRARIES (Run once if needed) ===\n",
    "# Description: This section lists the necessary Python libraries.\n",
    "#              Uncomment and run the '!pip install' line in your environment\n",
    "#              (like a Jupyter notebook or terminal) if these libraries are not yet installed.\n",
    "# !pip install requests openpyxl tqdm pandas gprofiler-official\n",
    "\n",
    "# === 2. IMPORT REQUIRED LIBRARIES ===\n",
    "# Description: Imports all the necessary libraries for the script's functionality.\n",
    "#              These include file handling (os), time delays (time), data manipulation (pandas, numpy),\n",
    "#              web requests (requests), progress bars (tqdm), gene enrichment (gprofiler),\n",
    "#              robust web request handling (HTTPAdapter, Retry), URL encoding (quote),\n",
    "#              date/time operations (datetime), logging (logging), type hints (typing),\n",
    "#              and regular expressions (re).\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from tqdm import tqdm  # For progress bars\n",
    "from gprofiler import GProfiler # For gene set enrichment analysis\n",
    "from requests.adapters import HTTPAdapter # For configuring advanced request options\n",
    "from urllib3.util.retry import Retry # For automatic retries on failed requests\n",
    "from urllib.parse import quote # For URL encoding gene symbols if needed (not directly used here but good practice)\n",
    "# import warnings # No longer strictly needed as warnings.warn replaced by logging\n",
    "from datetime import datetime # For timestamping output files and logs\n",
    "import logging  # Added for structured logging of script progress and errors\n",
    "import typing  # Added for type hinting (improves code readability and maintainability)\n",
    "import re # Added for regex operations (e.g., cleaning sheet names, keyword checks)\n",
    "\n",
    "# === 3. CONFIGURATION SETTINGS ===\n",
    "# Description: Centralized configuration for file paths, API parameters, and analysis settings.\n",
    "#              Modify these values according to your environment and analysis needs.\n",
    "\n",
    "# --- User Configuration ---\n",
    "# !! IMPORTANT: Set these paths correctly for your system !!\n",
    "# INPUT_EXCEL_PATH: Path to the input Excel file containing protein/gene lists.\n",
    "#                   Consider using command-line arguments or relative paths for better portability.\n",
    "INPUT_EXCEL_PATH = \"/Users/umuttank/Downloads/Results.xlsx\"\n",
    "# PFAM_CLAN_FILE_PATH: Path to the Pfam Clan TSV file (compressed or uncompressed).\n",
    "#                      Download from Pfam FTP: ftp://ftp.ebi.ac.uk/pub/databases/Pfam/current_release/Pfam-A.clans.tsv.gz\n",
    "PFAM_CLAN_FILE_PATH = \"Pfam-A.clans.tsv.gz\"\n",
    "# --- End User Configuration ---\n",
    "\n",
    "# --- Dynamic Output Path ---\n",
    "# Generates the output file name based on the input name and the current date.\n",
    "today = datetime.today().strftime(\"%Y%m%d\")\n",
    "output_path = INPUT_EXCEL_PATH.replace(\".xlsx\", f\"_Annotated_{today}.xlsx\")\n",
    "\n",
    "# --- API and Analysis Parameters ---\n",
    "CONFIG = {\n",
    "    # UniProt API request settings (also used for STRING, QuickGO via the session)\n",
    "    \"uniprot\": {\n",
    "        \"max_retries\": 5,  # Max number of retry attempts (Total attempts = 1 + max_retries)\n",
    "        \"delay\": 0.4,      # Backoff factor (wait_time = delay * (2 ** num_retries))\n",
    "        \"timeout\": 20      # Seconds to wait for a response before timing out.\n",
    "    },\n",
    "    # g:Profiler enrichment analysis settings\n",
    "    \"gprofiler\": {\n",
    "        \"sources\": [\"GO:BP\", \"GO:MF\", \"GO:CC\", \"REAC\", \"KEGG\", \"WP\", \"CORUM\"], # Databases to query.\n",
    "        \"threshold\": 0.05,         # Significance threshold (p-value) for enrichment terms.\n",
    "        \"max_terms\": 5             # Maximum number of enriched terms to report per source per gene.\n",
    "    },\n",
    "    # Keywords used to flag potential \"sticky\" or common background proteins.\n",
    "    \"background_keywords\": [\n",
    "        \"ribosomal\", \"histone\", \"tubulin\", \"actin\", \"myosin\", \"keratin\",\n",
    "        \"elongation factor\", \"collagen\", \"fibrinogen\",\n",
    "        \"HSP\", \"heat shock protein\", \"GAPDH\", \"aldolase\", \"annexin\", \"vimentin\"\n",
    "    ],\n",
    "    # Column names in the input sheet used for EV LFQ background detection. Adjust if your names differ.\n",
    "    \"ev_columns\": [\"EV_01\", \"EV_02\", \"EV_03\", \"EV_04\"],\n",
    "    # Name of the sheet used to determine a list of general EV control background proteins.\n",
    "    \"ev_sheet_name\": \"EV vs EV_ISD\",\n",
    "    # \"ev_threshold_multiplier\": 2 # This key is defined but not used, can be removed if not needed\n",
    "}\n",
    "\n",
    "# === 4. COLUMN NAME CONSTANTS ===\n",
    "# Description: Defines constants for column names used throughout the script.\n",
    "#              This makes the code more readable and easier to maintain if column names change.\n",
    "\n",
    "# --- Input Columns (Expected in the input Excel file) ---\n",
    "COL_GENE_NAMES = \"Gene names\"         # Essential: Column containing gene symbols.\n",
    "COL_PROTEIN_NAMES = \"Protein names\"   # Optional: Column containing protein names (used for QC check).\n",
    "COL_SIGNIFICANT = \"Significant\"       # Optional: Column indicating significance (e.g., '+' for significant). If absent, all genes are processed.\n",
    "COL_LOG2FC = \"log2FC\"             # Optional: Column containing log2 Fold Change values (used for EV LFQ flag).\n",
    "\n",
    "# --- Added Columns (Generated by this script) ---\n",
    "COL_MAIN_GENE = \"Main Gene\"           # The primary gene symbol extracted from COL_GENE_NAMES.\n",
    "\n",
    "# Basic UniProt Annotations (fetched via UniProt REST API)\n",
    "COL_UNIPROT_ID = \"UniProt ID\"             # Primary UniProt accession number.\n",
    "COL_PROTEIN_DESC = \"Protein Description\"    # UniProt recommended full protein name.\n",
    "COL_FUNCTION = \"Function\"               # Functional annotation from UniProt comments.\n",
    "COL_LOCATION = \"Subcellular Location\"   # Subcellular location from UniProt comments.\n",
    "COL_CANONICAL = \"Canonical Isoform\"     # Heuristic check if the UniProt ID represents the canonical sequence.\n",
    "COL_MASS_KDA = \"Molecular Mass (kDa)\"   # Protein molecular weight calculated from sequence.\n",
    "\n",
    "# Pfam Annotations (derived from UniProt cross-references and Pfam clan file)\n",
    "COL_PFAM_DOMAINS = \"PFAM Domains\"           # Pfam domain IDs associated with the protein.\n",
    "COL_PFAM_DESC = \"PFAM Descriptions\"       # Descriptions of the associated Pfam domains.\n",
    "COL_PFAM_CLANS = \"PFAM Clan Names\"        # Names of Pfam clans the domains belong to.\n",
    "\n",
    "# Extended UniProt Annotations (additional fields extracted from UniProt entry)\n",
    "COL_ALT_NAMES_V2 = \"Alternative Names\"       # Alternative protein names from UniProt.\n",
    "COL_CAT_ACT_V2 = \"Catalytic Activity\"      # Catalytic activity details from UniProt comments.\n",
    "COL_DISEASES_V2 = \"Associated Diseases\"    # Disease associations from UniProt comments.\n",
    "COL_SIMILARITIES_V2 = \"Sequence Similarities\" # Sequence similarity details from UniProt comments.\n",
    "COL_PATHWAYS_V2 = \"UniProt Pathways\"       # Pathway information from UniProt comments.\n",
    "COL_EXISTENCE_V2 = \"Protein Existence Level\" # UniProt evidence code for protein existence.\n",
    "\n",
    "# Dynamically Generated UniProt Keyword Columns\n",
    "# Example: \"UniProt KW: Molecular_function\", \"UniProt KW: Biological_process\"\n",
    "\n",
    "# <<< REMOVED UniProt GO Term Columns >>>\n",
    "# COL_UNIPROT_GO_CC = \"UniProt GO:CC Terms\"\n",
    "\n",
    "# QuickGO Annotations (Fetched separately from QuickGO API)\n",
    "# Columns added dynamically in main(): \"QuickGO:BP Terms\", \"QuickGO:MF Terms\", \"QuickGO:CC Terms\"\n",
    "\n",
    "# STRING Annotations (Fetched from STRING DB API)\n",
    "COL_STRING_PHYS = \"STRING Physical Interactors\" # High-confidence physical interactors.\n",
    "COL_STRING_FUNC = \"STRING Functional Partners\"  # High-confidence functional partners.\n",
    "\n",
    "# g:Profiler Annotations (Column names are generated dynamically based on CONFIG[\"gprofiler\"][\"sources\"])\n",
    "# Example: \"GO:BP Terms\", \"REAC Terms\", etc.\n",
    "\n",
    "# Flag/QC Columns (Generated based on annotation data or input values)\n",
    "COL_STICKY_FLAG = \"Sticky Binder Flag\"        # Flagged if protein description contains background keywords.\n",
    "COL_EV_LFQ_FLAG = \"EV LFQ Background Flag\"    # Flagged based on EV intensities and log2FC criteria.\n",
    "COL_EV_BG_FLAG = \"EV Control Background Flag\" # Flagged if gene is significant in the specified EV control sheet.\n",
    "COL_NAME_MATCH = \"Name Match Discrepancy\"     # QC check: Flags potential mismatch between input name and UniProt name.\n",
    "\n",
    "# === 5. SETUP LOGGING ===\n",
    "# Description: Configures the logging system to record script progress, warnings, and errors.\n",
    "#              Logs are saved to a file and also printed to the console.\n",
    "\n",
    "log_file_name = f\"annotation_log_{today}.log\" # Log file named with the current date.\n",
    "\n",
    "# Check if the root logger already has handlers (e.g., from previous runs in an interactive environment like Jupyter).\n",
    "# If handlers exist, reconfigure the logger using force=True. Otherwise, set up basic configuration.\n",
    "if not logging.getLogger().hasHandlers():\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO, # Set level: INFO, DEBUG, WARNING, ERROR, CRITICAL\n",
    "        format=\"%(asctime)s [%(levelname)-8s] %(message)s\", # Define log message format.\n",
    "        datefmt=\"%Y-%m-%d %H:%M:%S\", # Define date format for log entries.\n",
    "        handlers=[\n",
    "            logging.FileHandler(log_file_name), # Log to a file.\n",
    "            logging.StreamHandler()             # Log to the console.\n",
    "        ]\n",
    "    )\n",
    "else:\n",
    "    # If handlers exist, reconfigure (force=True is important here).\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO, # Ensure level is consistent\n",
    "        format=\"%(asctime)s [%(levelname)-8s] %(message)s\",\n",
    "        datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "        handlers=[\n",
    "            logging.FileHandler(log_file_name),\n",
    "            logging.StreamHandler()\n",
    "        ],\n",
    "        force=True # Force reconfiguration if logger already exists.\n",
    "    )\n",
    "\n",
    "# Log initial script information.\n",
    "logging.info(f\"--- Script Execution Started ---\")\n",
    "logging.info(f\"Input file: {INPUT_EXCEL_PATH}\")\n",
    "logging.info(f\"Output file: {output_path}\")\n",
    "logging.info(f\"Pfam Clan file: {PFAM_CLAN_FILE_PATH}\")\n",
    "logging.info(f\"Log file: {log_file_name}\")\n",
    "\n",
    "# === 6. SETUP REQUESTS SESSION (Robust Retry) ===\n",
    "# Description: Configures a 'requests' session with automatic retries for network robustness.\n",
    "#              Used for UniProt, STRING, and QuickGO API calls.\n",
    "#              Helps handle temporary API server issues or rate limits.\n",
    "\n",
    "session = requests.Session() # Create a session object to persist parameters across requests.\n",
    "# Define the retry strategy using parameters from CONFIG.\n",
    "retry_strategy = Retry(\n",
    "    total=CONFIG[\"uniprot\"][\"max_retries\"],      # Max number of retries (e.g., 4)\n",
    "    backoff_factor=CONFIG[\"uniprot\"][\"delay\"],   # Controls wait time: wait = delay * (2**retry_num) (e.g., 0.5)\n",
    "    status_forcelist=[429, 500, 502, 503, 504], # Retry on these HTTP status codes.\n",
    "    allowed_methods=[\"HEAD\", \"GET\", \"OPTIONS\"] # Only retry on these safe methods.\n",
    ")\n",
    "# Create an HTTP adapter with the retry strategy.\n",
    "adapter = HTTPAdapter(max_retries=retry_strategy)\n",
    "# Mount the adapter to handle HTTPS (and optionally HTTP) requests made with this session.\n",
    "session.mount(\"https://\", adapter)\n",
    "session.mount(\"http://\", adapter) # Also mount for http if needed.\n",
    "# Log the configured retry settings.\n",
    "logging.info(f\"Requests session configured with max {CONFIG['uniprot']['max_retries']} retries and backoff factor {CONFIG['uniprot']['delay']}.\")\n",
    "\n",
    "# === 7. LOAD PFAM CLAN TABLE ===\n",
    "# Description: Loads the Pfam clan information from the specified TSV file.\n",
    "#              This data is used to map Pfam domain IDs (obtained from UniProt)\n",
    "#              to their descriptions and clan names. Handles file not found and parsing errors.\n",
    "\n",
    "pfam_clan_df = None # Initialize variable.\n",
    "try:\n",
    "    # Read the potentially gzipped TSV file into a pandas DataFrame.\n",
    "    pfam_clan_df = pd.read_csv(\n",
    "        PFAM_CLAN_FILE_PATH,\n",
    "        sep=\"\\t\",               # Tab-separated values.\n",
    "        header=None,            # No header row in the file.\n",
    "        names=[\"Pfam_ID\", \"Clan_ID\", \"Clan_Name\", \"Pfam_Name\", \"Pfam_Description\"], # Assign column names.\n",
    "        compression='gzip'      # Assumes gzip compression based on common practice/extension.\n",
    "    )\n",
    "    logging.info(f\"Successfully loaded Pfam clan data ({len(pfam_clan_df)} rows) from {PFAM_CLAN_FILE_PATH}\")\n",
    "except FileNotFoundError:\n",
    "    # Handle case where the Pfam file is not found; annotations will lack clan info.\n",
    "    logging.error(f\"Pfam clan file not found at: {PFAM_CLAN_FILE_PATH}. PFAM Clan/Description annotations will be incomplete.\")\n",
    "    pfam_clan_df = pd.DataFrame(columns=[\"Pfam_ID\", \"Clan_ID\", \"Clan_Name\", \"Pfam_Name\", \"Pfam_Description\"]) # Create empty df\n",
    "except Exception as e:\n",
    "    # Handle other potential errors during file loading/parsing.\n",
    "    logging.error(f\"Error loading or parsing Pfam clan file {PFAM_CLAN_FILE_PATH}: {e}\")\n",
    "    pfam_clan_df = pd.DataFrame(columns=[\"Pfam_ID\", \"Clan_ID\", \"Clan_Name\", \"Pfam_Name\", \"Pfam_Description\"]) # Create empty df\n",
    "\n",
    "# === 8. SETUP GLOBAL CACHES ===\n",
    "# Description: Initializes dictionaries used as caches to store results from API calls.\n",
    "#              This avoids redundant requests for the same gene symbol, speeding up the process\n",
    "#              and reducing load on the external APIs. Caches are cleared at the start if run via `if __name__ == \"__main__\":`.\n",
    "\n",
    "global_cache = {\n",
    "    \"uniprot_entry\": {},    # Stores the full JSON response from UniProt for each gene symbol.\n",
    "    \"uniprot_basic\": {},    # Stores extracted basic fields (ID, desc, function, etc.) from UniProt.\n",
    "    \"uniprot_extra\": {},    # Stores extracted extra fields (alt names, keywords, etc.) from UniProt. <<< No UniProt GO >>>\n",
    "    \"gprofiler\": {},        # Stores g:Profiler enrichment results for each gene symbol.\n",
    "    \"string_physical\": {},  # Stores STRING physical interactors for each gene symbol.\n",
    "    \"string_functional\": {},# Stores STRING functional partners for each gene symbol.\n",
    "    \"quickgo_cc\": {},       # Stores QuickGO Cellular Component terms for each gene symbol.\n",
    "    \"quickgo_mf\": {},       # Stores QuickGO Molecular Function terms for each gene symbol.\n",
    "    \"quickgo_bp\": {}        # Stores QuickGO Biological Process terms for each gene symbol.\n",
    "}\n",
    "logging.info(\"Initialized global caches for API results.\")\n",
    "\n",
    "\n",
    "# === QuickGO Fetching Function ===\n",
    "def fetch_quickgo_annotations(symbol: str) -> None:\n",
    "    \"\"\"\n",
    "    Fetches Gene Ontology (GO) term names (CC, MF, BP) for a given gene symbol\n",
    "    using the QuickGO REST API, based on its UniProt accession.\n",
    "\n",
    "    This function first gets the UniProt entry to find the accession, then queries\n",
    "    QuickGO for annotations associated with that accession, and finally fetches\n",
    "    the details (name, aspect) for each unique GO ID found. Uses the shared `session`.\n",
    "\n",
    "    Results are stored directly in the global_cache['quickgo_cc'],\n",
    "    global_cache['quickgo_mf'], and global_cache['quickgo_bp'] dictionaries,\n",
    "    keyed by the uppercase gene symbol.\n",
    "\n",
    "    :param symbol: The gene symbol (e.g., \"TP53\") to fetch annotations for.\n",
    "    :type symbol: str\n",
    "    :return: None. Results are stored in the global cache.\n",
    "    :rtype: None\n",
    "    \"\"\"\n",
    "    # Validate input symbol\n",
    "    if not symbol:\n",
    "        logging.warning(\"fetch_quickgo_annotations called with empty symbol.\")\n",
    "        return\n",
    "\n",
    "    # Use uppercase symbol consistent with cache keys\n",
    "    symbol_upper = symbol.upper()\n",
    "\n",
    "    # Check cache first (check one, assume all attempted if present)\n",
    "    if symbol_upper in global_cache[\"quickgo_cc\"]: # Removed check for mf/bp as cc implies attempt\n",
    "         logging.debug(f\"QuickGO annotations already cached for {symbol}.\")\n",
    "         return\n",
    "\n",
    "    logging.debug(f\"Fetching QuickGO annotations for {symbol}...\")\n",
    "\n",
    "    # Get UniProt entry to find the accession number (uses cache if available)\n",
    "    uniprot_entry = fetch_uniprot_entry(symbol)\n",
    "    if not uniprot_entry:\n",
    "        logging.error(f\"Cannot fetch QuickGO annotations: no UniProt entry found for {symbol}\")\n",
    "        global_cache[\"quickgo_cc\"][symbol_upper] = pd.NA\n",
    "        global_cache[\"quickgo_mf\"][symbol_upper] = pd.NA\n",
    "        global_cache[\"quickgo_bp\"][symbol_upper] = pd.NA\n",
    "        return\n",
    "\n",
    "    # Extract primary accession from the UniProt entry\n",
    "    acc = uniprot_entry.get(\"primaryAccession\")\n",
    "    if not acc:\n",
    "        logging.error(f\"Cannot fetch QuickGO annotations: no accession found in UniProt entry for {symbol}\")\n",
    "        global_cache[\"quickgo_cc\"][symbol_upper] = pd.NA\n",
    "        global_cache[\"quickgo_mf\"][symbol_upper] = pd.NA\n",
    "        global_cache[\"quickgo_bp\"][symbol_upper] = pd.NA\n",
    "        return\n",
    "\n",
    "    # Step 1: Get GO IDs associated with the UniProt accession from QuickGO\n",
    "    annotations_url = \"https://www.ebi.ac.uk/QuickGO/services/annotation/search\"\n",
    "    params = {\n",
    "        \"geneProductId\": f\"UniProtKB:{acc}\", # Query by UniProt accession\n",
    "        \"limit\": 200                       # Max number of annotations to retrieve\n",
    "    }\n",
    "    headers = {\"Accept\": \"application/json\"} # Request JSON format\n",
    "\n",
    "    try:\n",
    "        # Make the API request using the configured session (includes retry logic)\n",
    "        r = session.get(annotations_url, headers=headers, params=params, timeout=CONFIG[\"uniprot\"][\"timeout\"])\n",
    "        r.raise_for_status() # Raise an exception for bad status codes (4xx or 5xx)\n",
    "        annotations = r.json()\n",
    "        go_ids = set() # Use a set to store unique GO IDs\n",
    "\n",
    "        # Extract GO IDs from the results\n",
    "        for item in annotations.get(\"results\", []):\n",
    "            go_id = item.get(\"goId\")\n",
    "            if go_id:\n",
    "                go_ids.add(go_id)\n",
    "\n",
    "        if not go_ids:\n",
    "            logging.warning(f\"No QuickGO GO terms found for UniProt accession {acc} ({symbol}).\")\n",
    "            global_cache[\"quickgo_cc\"][symbol_upper] = pd.NA\n",
    "            global_cache[\"quickgo_mf\"][symbol_upper] = pd.NA\n",
    "            global_cache[\"quickgo_bp\"][symbol_upper] = pd.NA\n",
    "            return\n",
    "\n",
    "        # Step 2: For each unique GO ID, fetch its name and aspect (BP, MF, CC)\n",
    "        bp_terms = set()\n",
    "        mf_terms = set()\n",
    "        cc_terms = set()\n",
    "\n",
    "        logging.debug(f\"Found {len(go_ids)} unique GO IDs for {symbol}. Fetching term details...\")\n",
    "        # Loop through each unique GO ID found\n",
    "        for go_id in go_ids:\n",
    "            term_url = f\"https://www.ebi.ac.uk/QuickGO/services/ontology/go/terms/{go_id}\"\n",
    "            # Request term details using the configured session\n",
    "            term_resp = session.get(term_url, headers=headers, timeout=CONFIG[\"uniprot\"][\"timeout\"])\n",
    "\n",
    "            if not term_resp.ok:\n",
    "                logging.warning(f\"Failed to fetch QuickGO term details for {go_id} (Symbol: {symbol}, Status: {term_resp.status_code})\")\n",
    "                continue # Skip this term if fetching fails\n",
    "\n",
    "            term_data = term_resp.json()\n",
    "            results = term_data.get(\"results\", [])\n",
    "            if not results:\n",
    "                logging.warning(f\"No results found in QuickGO term details response for {go_id} (Symbol: {symbol})\")\n",
    "                continue # Skip if no results structure\n",
    "\n",
    "            # Extract name and aspect from the first result entry\n",
    "            term_info = results[0]\n",
    "            name = term_info.get(\"name\")\n",
    "            aspect = term_info.get(\"aspect\") # 'biological_process', 'molecular_function', or 'cellular_component'\n",
    "\n",
    "            if not name or not aspect:\n",
    "                logging.warning(f\"Missing name or aspect for GO term {go_id} (Symbol: {symbol})\")\n",
    "                continue # Skip if essential info is missing\n",
    "\n",
    "            # Add the term name to the appropriate set based on its aspect\n",
    "            if aspect == \"biological_process\":\n",
    "                bp_terms.add(name)\n",
    "            elif aspect == \"molecular_function\":\n",
    "                mf_terms.add(name)\n",
    "            elif aspect == \"cellular_component\":\n",
    "                cc_terms.add(name)\n",
    "\n",
    "            # Optional: Small delay can be added here if hitting rate limits *within* this loop specifically\n",
    "            # time.sleep(0.02) # Example: 20ms delay\n",
    "\n",
    "        # Store the collected terms in the global cache, joined by semicolons\n",
    "        global_cache[\"quickgo_cc\"][symbol_upper] = \"; \".join(sorted(cc_terms)) if cc_terms else pd.NA\n",
    "        global_cache[\"quickgo_mf\"][symbol_upper] = \"; \".join(sorted(mf_terms)) if mf_terms else pd.NA\n",
    "        global_cache[\"quickgo_bp\"][symbol_upper] = \"; \".join(sorted(bp_terms)) if bp_terms else pd.NA\n",
    "        logging.debug(f\"Successfully cached QuickGO annotations for {symbol}.\")\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        # Handle errors during the API requests\n",
    "        logging.error(f\"QuickGO API request failed for {symbol} (Accession: {acc}): {e}\")\n",
    "        global_cache[\"quickgo_cc\"][symbol_upper] = pd.NA\n",
    "        global_cache[\"quickgo_mf\"][symbol_upper] = pd.NA\n",
    "        global_cache[\"quickgo_bp\"][symbol_upper] = pd.NA\n",
    "    except Exception as e:\n",
    "        # Handle unexpected errors during processing\n",
    "        logging.error(f\"Unexpected error during QuickGO annotation fetching for {symbol} (Accession: {acc}): {e}\")\n",
    "        global_cache[\"quickgo_cc\"][symbol_upper] = pd.NA\n",
    "        global_cache[\"quickgo_mf\"][symbol_upper] = pd.NA\n",
    "        global_cache[\"quickgo_bp\"][symbol_upper] = pd.NA\n",
    "\n",
    "\n",
    "# === 9. UTILITY FUNCTIONS ===\n",
    "# Description: Helper functions for common data manipulation tasks like extracting gene symbols\n",
    "#              and processing Pfam domain information.\n",
    "\n",
    "def extract_main_gene_symbol(name: typing.Optional[str]) -> typing.Optional[str]:\n",
    "    \"\"\"\n",
    "    Extracts the primary gene symbol from a string, typically the first element\n",
    "    if the string is semicolon or space-separated. Returns the symbol in uppercase.\n",
    "    Handles potential NaN, None, or empty string inputs.\n",
    "\n",
    "    :param name: The input string, potentially containing multiple gene names.\n",
    "    :type name: typing.Optional[str]\n",
    "    :return: The extracted primary gene symbol in uppercase, or None if input is invalid/empty.\n",
    "    :rtype: typing.Optional[str]\n",
    "    \"\"\"\n",
    "    if pd.isna(name) or not isinstance(name, str) or not name.strip():\n",
    "        return None # Return None for NaN, non-string, or empty/whitespace-only strings.\n",
    "    # Replace semicolons with spaces, split by whitespace, take the first part, strip whitespace, and convert to uppercase.\n",
    "    return name.replace(';', ' ').split()[0].strip().upper()\n",
    "\n",
    "def extract_pfam_domains_from_uniprot_entry(entry: dict) -> str:\n",
    "    \"\"\"\n",
    "    Extracts Pfam domain IDs from the 'uniProtKBCrossReferences' section\n",
    "    of a UniProt API JSON entry.\n",
    "\n",
    "    :param entry: The UniProt entry JSON object (as a dictionary).\n",
    "    :type entry: dict\n",
    "    :return: A semicolon-separated string of unique, sorted Pfam IDs, or an empty string if none found.\n",
    "    :rtype: str\n",
    "    \"\"\"\n",
    "    pfams = set() # Use a set to automatically handle duplicates.\n",
    "    if not entry or not isinstance(entry, dict):\n",
    "        return \"\" # Return empty if the entry is missing or not a dictionary.\n",
    "\n",
    "    # Iterate through cross-references in the UniProt entry.\n",
    "    for ref in entry.get(\"uniProtKBCrossReferences\", []):\n",
    "        # Check if the reference is a dictionary, is from the 'Pfam' database, and has an 'id'.\n",
    "        if isinstance(ref, dict) and ref.get(\"database\") == \"Pfam\" and ref.get(\"id\"):\n",
    "            pfams.add(ref.get(\"id\")) # Add the Pfam ID to the set.\n",
    "\n",
    "    # Return the sorted Pfam IDs joined by semicolons.\n",
    "    return \"; \".join(sorted(list(pfams))) if pfams else \"\"\n",
    "\n",
    "def get_pfam_metadata(pfam_ids_string: typing.Optional[str]) -> typing.Tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Looks up Pfam descriptions and clan names for a given string of semicolon-separated Pfam IDs,\n",
    "    using the pre-loaded Pfam clan DataFrame (`pfam_clan_df`).\n",
    "\n",
    "    :param pfam_ids_string: A string containing one or more Pfam IDs separated by semicolons.\n",
    "    :type pfam_ids_string: typing.Optional[str]\n",
    "    :return: A tuple containing two strings:\n",
    "             1. Semicolon-separated unique Pfam descriptions.\n",
    "             2. Semicolon-separated unique Pfam clan names.\n",
    "             Returns (\"\", \"\") if input is invalid, no IDs match, or `pfam_clan_df` is unavailable.\n",
    "    :rtype: typing.Tuple[str, str]\n",
    "    \"\"\"\n",
    "    # Check if input string is valid and the Pfam DataFrame is loaded and not empty.\n",
    "    if not pfam_ids_string or pfam_clan_df is None or pfam_clan_df.empty:\n",
    "        return \"\", \"\"\n",
    "\n",
    "    try:\n",
    "        # Split the input string into a set of unique, stripped Pfam IDs.\n",
    "        pfam_ids = {pfam.strip() for pfam in pfam_ids_string.split(\";\") if pfam.strip()}\n",
    "        if not pfam_ids: return \"\", \"\" # Return empty if no valid IDs after splitting/stripping.\n",
    "\n",
    "        # Check if the required column exists in the DataFrame.\n",
    "        if \"Pfam_ID\" not in pfam_clan_df.columns:\n",
    "             logging.warning(\"Pfam_ID column not found in Pfam clan data. Cannot fetch metadata.\")\n",
    "             return \"\", \"\"\n",
    "\n",
    "        # Filter the Pfam DataFrame to find rows matching the extracted Pfam IDs.\n",
    "        matched = pfam_clan_df[pfam_clan_df[\"Pfam_ID\"].isin(pfam_ids)].copy()\n",
    "\n",
    "        # Extract unique, non-null descriptions and sort them.\n",
    "        descriptions = \"; \".join(sorted(matched[\"Pfam_Description\"].dropna().astype(str).unique()))\n",
    "        # Extract unique, non-null clan names and sort them.\n",
    "        clans = \"; \".join(sorted(matched[\"Clan_Name\"].dropna().astype(str).unique()))\n",
    "\n",
    "        return descriptions, clans\n",
    "    except Exception as e:\n",
    "        # Log any error during the lookup process.\n",
    "        logging.error(f\"Error getting Pfam metadata for IDs '{pfam_ids_string}': {e}\")\n",
    "        return \"\", \"\"\n",
    "\n",
    "# === 10. FETCH and PROCESS FUNCTIONS ===\n",
    "# Description: Functions dedicated to fetching data from external APIs (UniProt, STRING, g:Profiler)\n",
    "#              and processing the raw responses into structured information suitable for the output table.\n",
    "#              These functions utilize the global caches to store and retrieve results.\n",
    "\n",
    "# --- UniProt Data Fetching and Processing ---\n",
    "\n",
    "def fetch_uniprot_entry(symbol: typing.Optional[str]) -> typing.Optional[dict]:\n",
    "    \"\"\"\n",
    "    Fetches the full UniProt JSON entry for a given gene symbol from the UniProtKB database.\n",
    "    It specifically searches for reviewed (Swiss-Prot) entries for Homo sapiens (organism ID 9606).\n",
    "    Handles caching and potential multiple matches by preferring non-isoform entries.\n",
    "    Uses the shared `session` with retry logic.\n",
    "\n",
    "    :param symbol: The gene symbol (e.g., \"TP53\").\n",
    "    :type symbol: typing.Optional[str]\n",
    "    :return: The UniProt entry as a dictionary if found, otherwise None. Results are cached.\n",
    "    :rtype: typing.Optional[dict]\n",
    "    \"\"\"\n",
    "    if not symbol: return None # Return None if no symbol is provided.\n",
    "    symbol_upper = symbol.upper() # Use uppercase for consistent cache keys.\n",
    "\n",
    "    # Check cache first.\n",
    "    if symbol_upper in global_cache[\"uniprot_entry\"]:\n",
    "        return global_cache[\"uniprot_entry\"][symbol_upper]\n",
    "\n",
    "    logging.debug(f\"Fetching UniProt entry for {symbol}...\")\n",
    "    try:\n",
    "        # Construct the UniProt search query.\n",
    "        query = f\"gene_exact:{symbol_upper} AND organism_id:9606 AND reviewed:true\"\n",
    "        search_params = {\n",
    "            \"query\": query,\n",
    "            \"format\": \"json\",\n",
    "            \"fields\": \"accession\", # Only request the accession field initially for search.\n",
    "            \"size\": 2              # Limit results (helps identify multiple hits quickly).\n",
    "        }\n",
    "        search_url = \"https://rest.uniprot.org/uniprotkb/search\"\n",
    "\n",
    "        # Perform the search request using the configured session.\n",
    "        search_resp = session.get(search_url, params=search_params, timeout=CONFIG[\"uniprot\"][\"timeout\"])\n",
    "        search_resp.raise_for_status() # Check for HTTP errors.\n",
    "        results = search_resp.json().get(\"results\", [])\n",
    "\n",
    "        if not results:\n",
    "            # Log if no reviewed entry is found for the symbol.\n",
    "            logging.warning(f\"No reviewed UniProt entry found for gene symbol: {symbol}\")\n",
    "            global_cache[\"uniprot_entry\"][symbol_upper] = None; return None\n",
    "\n",
    "        acc = None # Variable to store the selected UniProt accession.\n",
    "        # Handle multiple search results.\n",
    "        if len(results) > 1:\n",
    "            # Prefer entries whose accession does not contain '-' (likely canonical forms).\n",
    "            non_isoform = [r for r in results if isinstance(r.get(\"primaryAccession\"), str) and '-' not in r.get(\"primaryAccession\")]\n",
    "            acc = non_isoform[0][\"primaryAccession\"] if non_isoform else results[0][\"primaryAccession\"]\n",
    "            logging.info(f\"Multiple UniProtKB hits for {symbol}. Using entry: {acc}\")\n",
    "        else:\n",
    "            # If only one result, use its accession.\n",
    "            acc = results[0][\"primaryAccession\"]\n",
    "\n",
    "        # Fetch the full entry using the selected accession.\n",
    "        entry_url = f\"https://rest.uniprot.org/uniprotkb/{acc}.json\"\n",
    "        entry_resp = session.get(entry_url, timeout=CONFIG[\"uniprot\"][\"timeout\"])\n",
    "        entry_resp.raise_for_status() # Check for HTTP errors on the entry request.\n",
    "        entry_data = entry_resp.json()\n",
    "\n",
    "        # Cache the fetched entry data.\n",
    "        global_cache[\"uniprot_entry\"][symbol_upper] = entry_data\n",
    "        logging.debug(f\"Successfully fetched/cached UniProt entry for {symbol} (Acc: {acc})\")\n",
    "        return entry_data\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        # Handle specific 404 errors vs general request errors.\n",
    "        if isinstance(e, requests.exceptions.HTTPError) and e.response.status_code == 404 and 'entry_url' in locals():\n",
    "             logging.warning(f\"UniProt entry request for {symbol} (Accession: {acc}) resulted in 404 Not Found. URL: {entry_url}\")\n",
    "        else:\n",
    "             logging.error(f\"UniProt request failed for {symbol}: {e}\")\n",
    "        global_cache[\"uniprot_entry\"][symbol_upper] = None; return None\n",
    "    except Exception as e:\n",
    "        # Handle any other unexpected errors.\n",
    "        logging.error(f\"Unexpected error fetching UniProt entry for {symbol}: {e}\")\n",
    "        global_cache[\"uniprot_entry\"][symbol_upper] = None; return None\n",
    "\n",
    "def process_uniprot_basic_fields(symbol: str, entry: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Processes a fetched UniProt JSON entry to extract a predefined set of basic annotation fields\n",
    "    (ID, Description, Function, Location, Pfam info, Mass, Canonical status).\n",
    "    Handles data extraction from nested JSON structures and looks up Pfam metadata.\n",
    "\n",
    "    :param symbol: The gene symbol (used for logging and cache keys).\n",
    "    :type symbol: str\n",
    "    :param entry: The UniProt entry dictionary obtained from `fetch_uniprot_entry`.\n",
    "    :type entry: dict\n",
    "    :return: A dictionary where keys are the basic column name constants (e.g., COL_UNIPROT_ID)\n",
    "             and values are the extracted data (or pd.NA if not found/applicable). Results are cached.\n",
    "    :rtype: dict\n",
    "    \"\"\"\n",
    "    symbol_upper = symbol.upper() # Use uppercase for cache keys.\n",
    "\n",
    "    # Check cache first.\n",
    "    if symbol_upper in global_cache[\"uniprot_basic\"]:\n",
    "        return global_cache[\"uniprot_basic\"][symbol_upper]\n",
    "\n",
    "    # Define the list of basic columns to populate.\n",
    "    basic_cols_list = [\n",
    "        COL_UNIPROT_ID, COL_PROTEIN_DESC, COL_FUNCTION, COL_LOCATION,\n",
    "        COL_PFAM_DOMAINS, COL_PFAM_DESC, COL_PFAM_CLANS, COL_CANONICAL, COL_MASS_KDA\n",
    "    ]\n",
    "    # Initialize the result dictionary with NA values.\n",
    "    result = {col: pd.NA for col in basic_cols_list}\n",
    "\n",
    "    # Return default NA values if the entry is missing or invalid.\n",
    "    if not entry or not isinstance(entry, dict):\n",
    "        logging.debug(f\"Cannot process basic fields for {symbol}, entry missing or invalid.\")\n",
    "        global_cache[\"uniprot_basic\"][symbol_upper] = result; return result\n",
    "\n",
    "    logging.debug(f\"Processing basic UniProt fields for {symbol}...\")\n",
    "    try:\n",
    "        # --- Extract Basic Fields ---\n",
    "        # UniProt ID (Primary Accession)\n",
    "        acc = entry.get(\"primaryAccession\", pd.NA)\n",
    "        result[COL_UNIPROT_ID] = acc\n",
    "\n",
    "        # Protein Description (Recommended Name, fallback to Submission Name)\n",
    "        name = entry.get(\"proteinDescription\", {}).get(\"recommendedName\", {}).get(\"fullName\", {}).get(\"value\", \"\")\n",
    "        if not name: # If recommendedName is not present, try the first submissionName\n",
    "            submission_names = entry.get(\"proteinDescription\", {}).get(\"submissionNames\", [])\n",
    "            if submission_names and isinstance(submission_names[0], dict):\n",
    "                 name = submission_names[0].get(\"fullName\", {}).get(\"value\", \"\")\n",
    "        result[COL_PROTEIN_DESC] = name if name else pd.NA\n",
    "\n",
    "        # Function (from FUNCTION comments)\n",
    "        function_text = pd.NA\n",
    "        for comment in entry.get(\"comments\", []):\n",
    "             if isinstance(comment, dict) and comment.get(\"commentType\") == \"FUNCTION\":\n",
    "                texts = comment.get(\"texts\")\n",
    "                # Extract the 'value' from the first text entry if available.\n",
    "                if isinstance(texts, list) and len(texts) > 0:\n",
    "                    first_text = texts[0]\n",
    "                    if isinstance(first_text, dict):\n",
    "                        function_text = first_text.get(\"value\", pd.NA)\n",
    "                        break # Found the function comment, no need to check others.\n",
    "        result[COL_FUNCTION] = function_text\n",
    "\n",
    "        # Molecular Mass (converted to kDa)\n",
    "        mass = entry.get(\"sequence\", {}).get(\"molWeight\", None)\n",
    "        mass_kda = pd.NA\n",
    "        if mass is not None:\n",
    "            try:\n",
    "                mass_kda = round(float(mass) / 1000, 2) # Calculate kDa and round.\n",
    "            except (ValueError, TypeError):\n",
    "                logging.warning(f\"Could not convert mass '{mass}' to float for {symbol}. Setting to NA.\")\n",
    "        result[COL_MASS_KDA] = mass_kda\n",
    "\n",
    "        # Subcellular Location (from SUBCELLULAR LOCATION comments)\n",
    "        locations = set() # Use set for unique locations.\n",
    "        for comment in entry.get(\"comments\", []):\n",
    "             if isinstance(comment, dict) and comment.get(\"commentType\") == \"SUBCELLULAR LOCATION\":\n",
    "                 locs_data = comment.get(\"subcellularLocations\", [])\n",
    "                 if isinstance(locs_data, list):\n",
    "                     for loc_entry in locs_data:\n",
    "                          if isinstance(loc_entry, dict):\n",
    "                              location_info = loc_entry.get(\"location\")\n",
    "                              if isinstance(location_info, dict):\n",
    "                                  value = location_info.get(\"value\")\n",
    "                                  if value: locations.add(value) # Add valid location value.\n",
    "        result[COL_LOCATION] = \"; \".join(sorted(list(locations))) if locations else pd.NA\n",
    "\n",
    "        # Pfam Domains (extracted using helper function)\n",
    "        pfam_str = extract_pfam_domains_from_uniprot_entry(entry)\n",
    "        result[COL_PFAM_DOMAINS] = pfam_str if pfam_str else pd.NA\n",
    "\n",
    "        # Pfam Descriptions and Clans (looked up using helper function)\n",
    "        pfam_desc, clan_name = get_pfam_metadata(pfam_str)\n",
    "        result[COL_PFAM_DESC] = pfam_desc if pfam_desc else pd.NA\n",
    "        result[COL_PFAM_CLANS] = clan_name if clan_name else pd.NA\n",
    "\n",
    "        # Canonical Isoform Check (heuristic based on accession format)\n",
    "        # Assumes accessions with '-' are isoforms, unless it's '-1' (often the reference).\n",
    "        is_canonical = \"No\" if isinstance(acc, str) and '-' in acc and '-1' not in acc else \"Yes\"\n",
    "        result[COL_CANONICAL] = is_canonical\n",
    "\n",
    "        # Cache the processed basic fields.\n",
    "        global_cache[\"uniprot_basic\"][symbol_upper] = result; return result\n",
    "\n",
    "    except Exception as e:\n",
    "        # Log errors during processing and return default NA values.\n",
    "        logging.error(f\"Error processing basic UniProt fields for {symbol}: {e}\")\n",
    "        result_error = {col: pd.NA for col in basic_cols_list}\n",
    "        global_cache[\"uniprot_basic\"][symbol_upper] = result_error; return result_error\n",
    "\n",
    "def process_uniprot_extra_fields(symbol: str, entry: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Processes a fetched UniProt JSON entry to extract additional annotation fields:\n",
    "    Alternative names, catalytic activity, diseases, similarities, pathways,\n",
    "    protein existence level, and categorized keywords.\n",
    "    <<< MODIFIED: No longer processes UniProt GO terms. >>>\n",
    "\n",
    "    Handles data extraction and formats results appropriately (e.g., semicolon-separated strings).\n",
    "    Dynamically creates column keys for categorized keywords (e.g., \"UniProt KW: Category\").\n",
    "\n",
    "    :param symbol: The gene symbol (used for logging and cache keys).\n",
    "    :type symbol: str\n",
    "    :param entry: The UniProt entry dictionary obtained from `fetch_uniprot_entry`.\n",
    "    :type entry: dict\n",
    "    :return: A dictionary where keys are the extra column name constants (e.g., COL_ALT_NAMES_V2)\n",
    "             or dynamically generated keyword column names, and values are the extracted data\n",
    "             (or pd.NA if not found/applicable). Results are cached.\n",
    "    :rtype: dict\n",
    "    \"\"\"\n",
    "    symbol_upper = symbol.upper() # Use uppercase for cache keys.\n",
    "\n",
    "    # Check cache first.\n",
    "    if symbol_upper in global_cache[\"uniprot_extra\"]:\n",
    "        return global_cache[\"uniprot_extra\"][symbol_upper]\n",
    "\n",
    "    # Define the *static* extra columns expected\n",
    "    # <<< MODIFIED: Removed UniProt GO columns >>>\n",
    "    static_extra_cols_keys = [\n",
    "        COL_ALT_NAMES_V2, COL_CAT_ACT_V2, COL_DISEASES_V2,\n",
    "        COL_SIMILARITIES_V2, COL_PATHWAYS_V2, COL_EXISTENCE_V2,\n",
    "    ]\n",
    "    # Initialize a dictionary to return in case of errors.\n",
    "    result_on_error = {col: pd.NA for col in static_extra_cols_keys}\n",
    "\n",
    "    # Return default NA values if the entry is missing or invalid.\n",
    "    if not entry or not isinstance(entry, dict):\n",
    "        logging.debug(f\"Cannot process extra fields for {symbol}, entry missing or invalid.\")\n",
    "        global_cache[\"uniprot_extra\"][symbol_upper] = result_on_error\n",
    "        return result_on_error\n",
    "\n",
    "    logging.debug(f\"Processing extra UniProt fields for {symbol}...\")\n",
    "    try:\n",
    "        results = {} # Initialize dictionary to store extracted extra fields.\n",
    "\n",
    "        # --- Alternative Names ---\n",
    "        alt_names = set()\n",
    "        protein_desc = entry.get(\"proteinDescription\", {})\n",
    "        rec_name_alts = protein_desc.get(\"alternativeNames\", [])\n",
    "        for alt in rec_name_alts:\n",
    "            if isinstance(alt, dict): val = alt.get(\"fullName\", {}).get(\"value\");\n",
    "            if val: alt_names.add(val)\n",
    "        sub_names = protein_desc.get(\"submissionNames\", [])\n",
    "        if len(sub_names) > 1:\n",
    "             for sub in sub_names[1:]:\n",
    "                 if isinstance(sub, dict): val = sub.get(\"fullName\", {}).get(\"value\");\n",
    "                 if val: alt_names.add(val)\n",
    "        results[COL_ALT_NAMES_V2] = \"; \".join(sorted(list(alt_names))) if alt_names else pd.NA\n",
    "\n",
    "        # --- Protein Existence Level ---\n",
    "        existence_mapping = {\n",
    "            \"ECO:0000269\": \"1: Evidence at protein level\", \"ECO:0000314\": \"1: Evidence at protein level\",\n",
    "            \"ECO:0000250\": \"1: Evidence at protein level\", \"ECO:0000303\": \"1: Evidence at protein level\",\n",
    "            \"ECO:0000305\": \"2: Evidence at transcript level\", \"ECO:0000255\": \"2: Evidence at transcript level\",\n",
    "            \"ECO:0000315\": \"2: Evidence at transcript level\", \"ECO:0000256\": \"3: Inferred from homology\",\n",
    "            \"ECO:0000313\": \"3: Inferred from homology\", \"ECO:0000307\": \"4: Predicted\",\n",
    "            \"ECO:0000213\": \"5: Uncertain\"\n",
    "        }\n",
    "        protein_existence_info = entry.get(\"proteinExistence\")\n",
    "        existence_str = pd.NA\n",
    "        if isinstance(protein_existence_info, dict):\n",
    "            protein_existence_code = protein_existence_info.get(\"evidenceCode\")\n",
    "            if protein_existence_code: existence_str = existence_mapping.get(protein_existence_code, protein_existence_code)\n",
    "        elif isinstance(protein_existence_info, str): existence_str = existence_mapping.get(protein_existence_info, protein_existence_info)\n",
    "        elif protein_existence_info is not None: logging.warning(f\"Unexpected type ({type(protein_existence_info).__name__}) for 'proteinExistence' in {symbol}. Value: '{protein_existence_info}'. Setting to NA.\")\n",
    "        results[COL_EXISTENCE_V2] = existence_str\n",
    "\n",
    "        # --- Process Comments (Catalytic Activity, Diseases, Similarities, Pathways) ---\n",
    "        catalytic_activity, diseases, similarities, pathways = set(), set(), set(), set()\n",
    "        for comment in entry.get(\"comments\", []):\n",
    "            if not isinstance(comment, dict): continue\n",
    "            ctype = comment.get(\"commentType\", \"\")\n",
    "            try:\n",
    "                if ctype == \"CATALYTIC ACTIVITY\":\n",
    "                    reaction = comment.get(\"reaction\", {})\n",
    "                    if isinstance(reaction, dict): name = reaction.get(\"name\");\n",
    "                    if name: catalytic_activity.add(name)\n",
    "                elif ctype == \"DISEASE\":\n",
    "                    disease_entry = comment.get(\"disease\", {})\n",
    "                    if isinstance(disease_entry, dict):\n",
    "                        disease_id, acronym, desc = disease_entry.get(\"diseaseId\"), disease_entry.get(\"acronym\"), disease_entry.get(\"description\")\n",
    "                        if disease_id and acronym: diseases.add(f\"{disease_id} ({acronym}): {desc}\" if desc else f\"{disease_id} ({acronym})\")\n",
    "                        elif desc: diseases.add(desc)\n",
    "                    elif comment.get(\"texts\"):\n",
    "                          for item in comment.get(\"texts\", []): val = item.get(\"value\") if isinstance(item, dict) else item if isinstance(item, str) else None;\n",
    "                          if val: diseases.add(str(val).strip())\n",
    "                elif ctype == \"SIMILARITY\":\n",
    "                     texts = comment.get(\"texts\")\n",
    "                     if isinstance(texts, list):\n",
    "                         for item in texts: val = item.get(\"value\") if isinstance(item, dict) else item if isinstance(item, str) else None;\n",
    "                         if val: similarities.add(str(val).strip())\n",
    "                elif ctype == \"PATHWAY\":\n",
    "                     texts = comment.get(\"texts\")\n",
    "                     if isinstance(texts, list):\n",
    "                         for item in texts: val = item.get(\"value\") if isinstance(item, dict) else item if isinstance(item, str) else None;\n",
    "                         if val: pathways.add(str(val).strip())\n",
    "            except Exception as com_e:\n",
    "                logging.warning(f\"Error processing comment type '{ctype}' for {symbol}: {com_e}\")\n",
    "        results[COL_CAT_ACT_V2] = \"; \".join(sorted(list(catalytic_activity))) if catalytic_activity else pd.NA\n",
    "        results[COL_DISEASES_V2] = \"; \".join(sorted(list(diseases))) if diseases else pd.NA\n",
    "        results[COL_SIMILARITIES_V2] = \"; \".join(sorted(list(similarities))) if similarities else pd.NA\n",
    "        results[COL_PATHWAYS_V2] = \"; \".join(sorted(list(pathways))) if pathways else pd.NA\n",
    "\n",
    "        # --- Process Categorized Keywords ---\n",
    "        categorized_keywords = {} # Dictionary to hold {Column Name: Keywords String}\n",
    "        raw_keywords = entry.get(\"keywords\", [])\n",
    "        if isinstance(raw_keywords, list):\n",
    "            temp_kw_dict = {} # Temporary dict {Category: set(Keywords)}\n",
    "            for kw_item in raw_keywords:\n",
    "                if isinstance(kw_item, dict):\n",
    "                    category = kw_item.get(\"category\")\n",
    "                    name = kw_item.get(\"name\") or kw_item.get(\"id\") # Use name, fallback to ID.\n",
    "                    if category and name:\n",
    "                        clean_category = category.replace(' ', '_') # Make category suitable for column name.\n",
    "                        if clean_category not in temp_kw_dict: temp_kw_dict[clean_category] = set()\n",
    "                        temp_kw_dict[clean_category].add(name)\n",
    "                    elif name: # Add keywords without a category to \"Uncategorized\".\n",
    "                        if \"Uncategorized\" not in temp_kw_dict: temp_kw_dict[\"Uncategorized\"] = set()\n",
    "                        temp_kw_dict[\"Uncategorized\"].add(name)\n",
    "            # Format the keyword columns for the final results dictionary.\n",
    "            for category, kw_set in temp_kw_dict.items():\n",
    "                col_name = f\"UniProt KW: {category}\" # Create dynamic column name.\n",
    "                categorized_keywords[col_name] = \"; \".join(sorted(list(kw_set)))\n",
    "        # Add the generated keyword columns to the main results dictionary.\n",
    "        results.update(categorized_keywords)\n",
    "\n",
    "        # <<< REMOVED UniProt GO Term Processing Section >>>\n",
    "        # The loop iterating through `uniProtKBCrossReferences` for GO terms is no longer needed here.\n",
    "\n",
    "        # --- Final Cache and Return ---\n",
    "        # Ensure all *statically* defined extra columns are present in the results, even if empty.\n",
    "        for key in static_extra_cols_keys:\n",
    "            if key not in results:\n",
    "                results[key] = pd.NA\n",
    "\n",
    "        # Cache the processed extra fields.\n",
    "        global_cache[\"uniprot_extra\"][symbol_upper] = results\n",
    "        return results\n",
    "\n",
    "    except Exception as e:\n",
    "        # Log errors during processing. Use `logging.exception` to include traceback.\n",
    "        logging.exception(f\"Error processing extra UniProt fields for {symbol}: {e}\")\n",
    "        # Cache the error state (NA values for static columns).\n",
    "        global_cache[\"uniprot_extra\"][symbol_upper] = result_on_error\n",
    "        return result_on_error\n",
    "\n",
    "\n",
    "# --- STRING DB Data Fetching ---\n",
    "\n",
    "def fetch_string_interactors(symbol: typing.Optional[str], network_type: str = \"physical\") -> str:\n",
    "    \"\"\"\n",
    "    Fetches high-confidence (score >= 700) interacting proteins for a given gene symbol\n",
    "    from the STRING database API (v11 or later). Uses the shared `session`.\n",
    "\n",
    "    :param symbol: The gene symbol (e.g., \"TP53\").\n",
    "    :type symbol: typing.Optional[str]\n",
    "    :param network_type: Type of interaction network ('physical' or 'functional'). Defaults to 'physical'.\n",
    "    :type network_type: str\n",
    "    :return: A semicolon-separated string of unique, sorted interactor gene symbols, or an empty string\n",
    "             if no interactors are found or an error occurs. Results are cached.\n",
    "    :rtype: str\n",
    "    \"\"\"\n",
    "    if not symbol: return \"\" # Return empty if no symbol provided.\n",
    "    symbol_upper = symbol.upper() # Use uppercase for consistency.\n",
    "    cache_key = f\"string_{network_type}\" # Dynamic cache key based on network type.\n",
    "\n",
    "    # Check cache first.\n",
    "    if symbol_upper in global_cache[cache_key]:\n",
    "        return global_cache[cache_key][symbol_upper]\n",
    "\n",
    "    logging.debug(f\"Fetching STRING ({network_type}) interactors for {symbol}...\")\n",
    "    result_on_error = \"\" # Default return value on error.\n",
    "\n",
    "    try:\n",
    "        # STRING API endpoint for network interactions.\n",
    "        url = \"https://string-db.org/api/tsv/network\"\n",
    "        # Parameters for the API request.\n",
    "        params = {\n",
    "            \"identifiers\": symbol,         # The query gene symbol.\n",
    "            \"species\": 9606,               # Homo sapiens NCBI taxonomy ID.\n",
    "            \"required_score\": 700,         # Minimum interaction score (0-1000, 700=high confidence).\n",
    "            \"network_type\": network_type,  # 'physical' or 'functional'.\n",
    "            \"limit\": 200,                  # Maximum number of interactors to return.\n",
    "            \"caller_identity\": \"PythonProteomicsScript_UL82\" # Identify the script to STRING DB admins.\n",
    "        }\n",
    "\n",
    "        # Make the request using the configured session.\n",
    "        resp = session.get(url, params=params, timeout=CONFIG[\"uniprot\"][\"timeout\"])\n",
    "        resp.raise_for_status() # Check for HTTP errors.\n",
    "\n",
    "        interactors = set() # Use a set for unique interactors.\n",
    "        lines = resp.text.strip().split(\"\\n\") # Split TSV response into lines.\n",
    "\n",
    "        # Process lines (skip header line[0]).\n",
    "        if len(lines) > 1:\n",
    "            query_symbol_upper_check = symbol.upper() # Ensure case-insensitive comparison.\n",
    "            for line in lines[1:]:\n",
    "                parts = line.split(\"\\t\")\n",
    "                # Ensure the line has enough columns (at least 4 for protein1, protein2).\n",
    "                if len(parts) >= 4:\n",
    "                    # Preferred names are usually in columns 2 and 3 (0-indexed).\n",
    "                    name1_api, name2_api = parts[2], parts[3] # Get potential interactor names\n",
    "                    name1_upper, name2_upper = name1_api.upper(), name2_api.upper() # Uppercase for comparison\n",
    "\n",
    "                    # Add the interactor that is *not* the query symbol itself.\n",
    "                    if name1_upper == query_symbol_upper_check and name2_upper != query_symbol_upper_check:\n",
    "                        interactors.add(name2_api) # Add the original case name\n",
    "                    elif name2_upper == query_symbol_upper_check and name1_upper != query_symbol_upper_check:\n",
    "                        interactors.add(name1_api) # Add the original case name\n",
    "\n",
    "        # Format the result string.\n",
    "        result = \"; \".join(sorted(list(interactors)))\n",
    "\n",
    "        # Cache the result.\n",
    "        global_cache[cache_key][symbol_upper] = result\n",
    "        logging.debug(f\"Successfully fetched {len(interactors)} STRING ({network_type}) interactors for {symbol}.\")\n",
    "        return result\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        # Log request errors.\n",
    "        logging.error(f\"STRING ({network_type}) request failed for {symbol}: {e}\")\n",
    "        global_cache[cache_key][symbol_upper] = result_on_error; return result_on_error\n",
    "    except Exception as e:\n",
    "        # Log other unexpected errors during processing.\n",
    "        logging.error(f\"Unexpected error processing STRING ({network_type}) data for {symbol}: {e}\")\n",
    "        global_cache[cache_key][symbol_upper] = result_on_error; return result_on_error\n",
    "\n",
    "\n",
    "# --- g:Profiler Enrichment Analysis ---\n",
    "\n",
    "# Initialize the g:Profiler client.\n",
    "try:\n",
    "    # Update user agent to be more specific\n",
    "    gp = GProfiler(return_dataframe=True, user_agent=\"PythonProteomicsScript_UL82/1.2\")\n",
    "    logging.info(\"g:Profiler client initialized.\")\n",
    "    gprofiler_available = True # Flag indicating g:Profiler is ready.\n",
    "except Exception as e:\n",
    "    logging.error(f\"Failed to initialize g:Profiler client: {e}. g:Profiler annotations will be skipped.\")\n",
    "    gp = None; gprofiler_available = False # Set flag to False if initialization fails.\n",
    "\n",
    "def fetch_gprofiler_enrichment(symbol: typing.Optional[str]) -> dict:\n",
    "    \"\"\"\n",
    "    Performs functional enrichment analysis for a *single* gene symbol using the g:Profiler API.\n",
    "    Queries the sources defined in CONFIG['gprofiler']['sources'] and filters by the p-value threshold.\n",
    "    Does NOT use the shared session's retry logic; relies on the gprofiler library's handling.\n",
    "\n",
    "    Note: g:Profiler is typically used for gene *lists*, but here it's applied per gene\n",
    "          to get associated terms from various databases (GO, KEGG, REAC, etc.) without\n",
    "          calculating enrichment significance in the traditional sense.\n",
    "\n",
    "    :param symbol: The gene symbol (e.g., \"TP53\").\n",
    "    :type symbol: typing.Optional[str]\n",
    "    :return: A dictionary where keys are formatted source names (e.g., \"GO:BP Terms\") and\n",
    "             values are semicolon-separated strings of the top associated term names (up to max_terms),\n",
    "             or pd.NA if no terms are found for a source or if g:Profiler is unavailable/fails.\n",
    "             Results are cached.\n",
    "    :rtype: dict\n",
    "    \"\"\"\n",
    "    # Return empty if no symbol or g:Profiler client isn't available.\n",
    "    if not symbol or not gprofiler_available: return {}\n",
    "    symbol_upper = symbol.upper() # Use uppercase for cache keys.\n",
    "\n",
    "    # Check cache first.\n",
    "    if symbol_upper in global_cache[\"gprofiler\"]:\n",
    "        return global_cache[\"gprofiler\"][symbol_upper]\n",
    "\n",
    "    logging.debug(f\"Fetching g:Profiler enrichment for {symbol}...\")\n",
    "    gprofiler_sources = CONFIG[\"gprofiler\"][\"sources\"]\n",
    "    # Define the structure for failure cases (all sources NA).\n",
    "    enrichment_failure = {f\"{s} Terms\": pd.NA for s in gprofiler_sources}\n",
    "\n",
    "    try:\n",
    "        # Perform the profile request using the g:Profiler client.\n",
    "        results_df = gp.profile(\n",
    "            organism=\"hsapiens\",                 # Human organism.\n",
    "            query=[symbol],                      # The single gene symbol as a list.\n",
    "            no_evidences=False,                  # Include evidence codes (though not explicitly used here).\n",
    "            user_threshold=CONFIG[\"gprofiler\"][\"threshold\"], # Significance threshold.\n",
    "            sources=gprofiler_sources,           # Databases to query.\n",
    "            all_results=False                    # Get only significant results based on threshold.\n",
    "        )\n",
    "\n",
    "        # Process the resulting DataFrame.\n",
    "        if results_df is None or results_df.empty:\n",
    "            # If no significant terms are found, use the failure structure.\n",
    "            enrichment = enrichment_failure\n",
    "        else:\n",
    "            enrichment = {}\n",
    "            # Iterate through each requested source (e.g., GO:BP, REAC).\n",
    "            for source in gprofiler_sources:\n",
    "                # Filter results for the current source and sort by p-value.\n",
    "                source_results = results_df[results_df[\"source\"] == source].sort_values(by=\"p_value\")\n",
    "                # Get the top 'max_terms' unique term names.\n",
    "                terms = source_results[\"name\"].dropna().unique().tolist()[:CONFIG[\"gprofiler\"][\"max_terms\"]]\n",
    "                # Format the column name and join terms with semicolons.\n",
    "                enrichment[f\"{source} Terms\"] = \"; \".join(terms) if terms else pd.NA\n",
    "            # Ensure all requested source columns exist in the final dict, even if empty.\n",
    "            for source in gprofiler_sources:\n",
    "                if f\"{source} Terms\" not in enrichment:\n",
    "                    enrichment[f\"{source} Terms\"] = pd.NA\n",
    "\n",
    "        # Cache the results.\n",
    "        global_cache[\"gprofiler\"][symbol_upper] = enrichment\n",
    "        logging.debug(f\"Successfully fetched/cached g:Profiler data for {symbol}.\")\n",
    "        return enrichment\n",
    "\n",
    "    except Exception as e:\n",
    "        # Log errors during the g:Profiler request or processing.\n",
    "        logging.error(f\"g:Profiler fetch/processing failed for {symbol}: {e}\")\n",
    "        # Cache the failure state.\n",
    "        global_cache[\"gprofiler\"][symbol_upper] = enrichment_failure; return enrichment_failure\n",
    "\n",
    "\n",
    "# === 11. BACKGROUND DETECTION FUNCTIONS ===\n",
    "# Description: Functions to apply heuristic flags for identifying potential background proteins\n",
    "#              based on keywords or expression patterns in control experiments (EVs).\n",
    "\n",
    "def detect_sticky_binder(protein_desc: typing.Optional[str]) -> str:\n",
    "    \"\"\"\n",
    "    Checks if a protein description contains any keywords from the predefined\n",
    "    `CONFIG[\"background_keywords\"]` list (case-insensitive).\n",
    "\n",
    "    :param protein_desc: The protein description string (usually from UniProt).\n",
    "    :type protein_desc: typing.Optional[str]\n",
    "    :return: \"Sticky Background\" if a keyword is found, otherwise an empty string.\n",
    "    :rtype: str\n",
    "    \"\"\"\n",
    "    # Return empty if description is missing, NaN, or not a string.\n",
    "    if pd.isna(protein_desc) or not isinstance(protein_desc, str): return \"\"\n",
    "\n",
    "    desc_lower = protein_desc.lower() # Convert to lowercase for case-insensitive matching.\n",
    "    # Check for each keyword in the description.\n",
    "    for keyword in CONFIG[\"background_keywords\"]:\n",
    "        if keyword.lower() in desc_lower:\n",
    "            return \"Sticky Background\" # Return flag immediately if found.\n",
    "    return \"\" # Return empty if no keywords match.\n",
    "\n",
    "def detect_ev_lfq_background(row: pd.Series) -> str:\n",
    "    \"\"\"\n",
    "    Flags proteins as potential Extracellular Vesicle (EV) background based on\n",
    "    LFQ intensities in EV control columns and the log2 Fold Change (log2FC).\n",
    "\n",
    "    The logic is: If the median intensity across specified EV columns (`CONFIG[\"ev_columns\"]`)\n",
    "    is > 0 AND the log2FC is < 1.0 (or missing/NaN), flag as \"Likely EV Background\".\n",
    "\n",
    "    Requires `CONFIG[\"ev_columns\"]` and `COL_LOG2FC` to be present in the input Series (row).\n",
    "\n",
    "    :param row: A pandas Series representing a row from the DataFrame.\n",
    "    :type row: pd.Series\n",
    "    :return: \"Likely EV Background\" if conditions are met, otherwise an empty string.\n",
    "    :rtype: str\n",
    "    \"\"\"\n",
    "    ev_cols = CONFIG[\"ev_columns\"] # Get EV column names from config.\n",
    "    # Check if all required EV columns and the log2FC column exist in the row's index.\n",
    "    if not set(ev_cols).issubset(row.index):\n",
    "        return \"\" # Silently return empty if columns are missing\n",
    "    if COL_LOG2FC not in row.index:\n",
    "        return \"\" # Silently return empty if columns are missing\n",
    "\n",
    "    try:\n",
    "        # Calculate the median of the EV columns, coercing errors to NaN.\n",
    "        median_ev = pd.to_numeric(row[ev_cols], errors='coerce').median()\n",
    "        # Get the log2FC value, coercing errors to NaN.\n",
    "        log2fc = pd.to_numeric(row.get(COL_LOG2FC), errors='coerce')\n",
    "\n",
    "        # Apply the background logic.\n",
    "        if pd.notna(median_ev) and median_ev > 0 and (pd.isna(log2fc) or log2fc < 1.0):\n",
    "            return \"Likely EV Background\"\n",
    "        return \"\" # Return empty if conditions are not met.\n",
    "    except Exception as e:\n",
    "        # Log errors during the calculation.\n",
    "        logging.error(f\"Error during EV LFQ background detection for row (Index: {row.name}): {e}\")\n",
    "        return \"\" # Return empty on error.\n",
    "\n",
    "\n",
    "# === 12. IMMEDIATE OUTPUT SAFETY FUNCTION ===\n",
    "# Description: A function designed to write DataFrames to Excel sheets robustly,\n",
    "#              handling potential issues like invalid characters in sheet names,\n",
    "#              data type problems that prevent writing, and ensuring the file is saved\n",
    "#              after each sheet write (for partial results in case of later errors).\n",
    "\n",
    "def write_sheet_safely(writer: pd.ExcelWriter, sheet_name: str, df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Writes a DataFrame to a specified sheet in an Excel file using an existing ExcelWriter.\n",
    "\n",
    "    Includes safety checks:\n",
    "    - Cleans the sheet name (removes invalid chars, truncates to 31 chars).\n",
    "    - Attempts to convert object columns to strings before writing.\n",
    "    - Saves the workbook immediately after writing the sheet.\n",
    "    - Logs progress and errors.\n",
    "\n",
    "    :param writer: The pandas ExcelWriter object.\n",
    "    :type writer: pd.ExcelWriter\n",
    "    :param sheet_name: The desired name for the sheet.\n",
    "    :type sheet_name: str\n",
    "    :param df: The pandas DataFrame to write.\n",
    "    :type df: pd.DataFrame\n",
    "    :return: None\n",
    "    :rtype: None\n",
    "    \"\"\"\n",
    "    # Define regex for characters invalid in Excel sheet names.\n",
    "    invalid_excel_chars = r'[\\\\*?:/\\[\\]]'\n",
    "    # Remove invalid characters and truncate sheet name to Excel's limit (31 chars).\n",
    "    safe_sheet_name = re.sub(invalid_excel_chars, '_', sheet_name)[:31]\n",
    "\n",
    "    try:\n",
    "        logging.info(f\"Writing sheet '{safe_sheet_name}' ({df.shape[0]} rows, {df.shape[1]} cols).\")\n",
    "        # Work on a copy to avoid modifying the original DataFrame.\n",
    "        df_copy = df.copy()\n",
    "\n",
    "        # Attempt to convert object columns to strings to prevent potential write errors.\n",
    "        for col in df_copy.select_dtypes(include=['object']).columns:\n",
    "            try:\n",
    "                # Fill NaNs with empty string and convert column to string type.\n",
    "                df_copy[col] = df_copy[col].fillna('').astype(str)\n",
    "            except Exception as e:\n",
    "                # If bulk conversion fails, try element-wise conversion as a fallback.\n",
    "                logging.warning(f\"Could not bulk convert column '{col}' to string in sheet '{safe_sheet_name}': {e}. Trying element-wise.\")\n",
    "                df_copy[col] = df_copy[col].apply(lambda x: str(x) if pd.notna(x) else '')\n",
    "\n",
    "        # Crucial check: Ensure the writer's underlying workbook object exists.\n",
    "        if not hasattr(writer, 'book') or writer.book is None:\n",
    "            logging.error(f\"ExcelWriter's 'book' attribute is missing or None. Cannot write sheet '{safe_sheet_name}'.\")\n",
    "            return # Cannot proceed without a valid workbook.\n",
    "\n",
    "        # Write the DataFrame to the Excel sheet.\n",
    "        df_copy.to_excel(\n",
    "            writer,\n",
    "            sheet_name=safe_sheet_name,\n",
    "            index=False,        # Do not write DataFrame index as a column.\n",
    "            freeze_panes=(1, 0) # Freeze the header row.\n",
    "        )\n",
    "\n",
    "        # Save the workbook immediately after writing the sheet.\n",
    "        # Ensures partial results if script fails later. Can be slow.\n",
    "        writer.book.save(output_path)\n",
    "        logging.info(f\"Successfully wrote and saved sheet '{safe_sheet_name}' to {output_path}.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        # Log any exceptions during writing or saving.\n",
    "        logging.exception(f\"Failed to write or save sheet '{safe_sheet_name}': {e}\")\n",
    "\n",
    "\n",
    "# === 13. MAIN PROCESSING FUNCTION ===\n",
    "# Description: Orchestrates the entire annotation process:\n",
    "#              - Loads the input Excel file.\n",
    "#              - Sets up the output Excel writer.\n",
    "#              - Pre-loads background gene lists if configured.\n",
    "#              - Iterates through each sheet in the input file.\n",
    "#              - For each sheet:\n",
    "#                  - Checks for presence of 'Significant' column.\n",
    "#                  - If 'Significant' is present, filters significant proteins/genes (+).\n",
    "#                  - If 'Significant' is absent, processes all rows with gene names.\n",
    "#                  - Fetches annotations (UniProt, STRING, g:Profiler, QuickGO) for unique selected genes, using caches.\n",
    "#                  - Maps fetched annotations back to the DataFrame.\n",
    "#                  - Applies background flags and QC checks.\n",
    "#                  - Reorders columns for readability.\n",
    "#                  - Writes the annotated data to a sheet in the output file using `write_sheet_safely`.\n",
    "#              - Finalizes and saves the output Excel file.\n",
    "# <<< MODIFIED: Description updated to reflect conditional logic and removal of UniProt GO >>>\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function orchestrating the loading, processing, annotation, and saving.\n",
    "    Handles sheets with or without a 'Significant' column for filtering.\n",
    "    Does NOT fetch GO terms directly from UniProt (uses QuickGO instead).\n",
    "    \"\"\"\n",
    "    logging.info(f\"--- Starting Main Processing ---\")\n",
    "\n",
    "    # --- Load Input Excel File ---\n",
    "    # Description: Attempts to load the Excel file specified by INPUT_EXCEL_PATH.\n",
    "    #              Logs the sheets found or aborts if the file is not found or fails to load.\n",
    "    try:\n",
    "        xlsx = pd.ExcelFile(INPUT_EXCEL_PATH)\n",
    "        sheet_names = xlsx.sheet_names\n",
    "        logging.info(f\"Loaded input Excel: {INPUT_EXCEL_PATH} with sheets: {sheet_names}\")\n",
    "    except FileNotFoundError:\n",
    "        logging.error(f\"CRITICAL: Input file not found at {INPUT_EXCEL_PATH}. Aborting.\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        logging.error(f\"CRITICAL: Failed loading input {INPUT_EXCEL_PATH}: {e}. Aborting.\")\n",
    "        return\n",
    "\n",
    "    # --- Setup Excel Writer ---\n",
    "    # Description: Initializes the pandas ExcelWriter object used to create the output file.\n",
    "    #              Creates the output directory if it doesn't exist. Aborts if the writer fails.\n",
    "    writer = None\n",
    "    try:\n",
    "        output_dir = os.path.dirname(output_path)\n",
    "        if output_dir and not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "            logging.info(f\"Created output directory: {output_dir}\")\n",
    "        writer = pd.ExcelWriter(output_path, engine=\"openpyxl\")\n",
    "        logging.info(f\"Initialized Excel writer for {output_path}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"CRITICAL: Failed initializing writer for {output_path}: {e}. Aborting.\")\n",
    "        if writer: writer.close()\n",
    "        return\n",
    "\n",
    "    # --- Pre-load EV Background Genes (Optional) ---\n",
    "    # Description: Loads a set of gene symbols from a specified control sheet (defined in CONFIG)\n",
    "    #              that are considered background contaminants in EV experiments. These genes will be flagged later.\n",
    "    ev_background_genes: typing.Set[str] = set()\n",
    "    ev_sheet_name = CONFIG.get(\"ev_sheet_name\")\n",
    "    if ev_sheet_name:\n",
    "        if ev_sheet_name in sheet_names:\n",
    "            try:\n",
    "                logging.info(f\"Loading EV background data from sheet: '{ev_sheet_name}'\")\n",
    "                df_ev_background = xlsx.parse(ev_sheet_name)\n",
    "                if COL_SIGNIFICANT in df_ev_background.columns and COL_GENE_NAMES in df_ev_background.columns:\n",
    "                    sig_genes = df_ev_background[df_ev_background[COL_SIGNIFICANT].astype(str).str.strip() == \"+\"][COL_GENE_NAMES].map(extract_main_gene_symbol)\n",
    "                    ev_background_genes = {gene for gene in sig_genes if pd.notna(gene)}\n",
    "                    logging.info(f\"Loaded {len(ev_background_genes)} unique background genes from '{ev_sheet_name}'.\")\n",
    "                else:\n",
    "                    logging.warning(f\"EV sheet '{ev_sheet_name}' missing required columns ({COL_SIGNIFICANT}, {COL_GENE_NAMES}). EV Control Background flag cannot be applied.\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing EV sheet '{ev_sheet_name}': {e}. EV Control Background flag may be incomplete.\")\n",
    "        else:\n",
    "            logging.warning(f\"Specified EV sheet '{ev_sheet_name}' not found in the input Excel file. EV Control Background flag cannot be applied.\")\n",
    "    else:\n",
    "        logging.info(\"No EV background sheet specified (CONFIG['ev_sheet_name']). Skipping EV Control Background flag based on sheet.\")\n",
    "\n",
    "\n",
    "    # --- Define Column Structure Lists (Static Parts) ---\n",
    "    # Description: Pre-defines lists of column names for different annotation categories.\n",
    "    #              This helps in organizing the mapping process and the final column order.\n",
    "    gprofiler_cols = [f\"{s} Terms\" for s in CONFIG[\"gprofiler\"][\"sources\"]]\n",
    "    uniprot_basic_cols = [ COL_UNIPROT_ID, COL_PROTEIN_DESC, COL_FUNCTION, COL_LOCATION, COL_PFAM_DOMAINS, COL_PFAM_DESC, COL_PFAM_CLANS, COL_CANONICAL, COL_MASS_KDA ]\n",
    "    uniprot_extra_static_cols = [ COL_ALT_NAMES_V2, COL_CAT_ACT_V2, COL_DISEASES_V2, COL_SIMILARITIES_V2, COL_PATHWAYS_V2, COL_EXISTENCE_V2 ]\n",
    "    uniprot_go_cols = [] # <<< MODIFIED: List is now empty as UniProt GO terms are removed >>>\n",
    "    quickgo_cols = [\"QuickGO:CC Terms\", \"QuickGO:MF Terms\", \"QuickGO:BP Terms\"]\n",
    "    string_cols = [COL_STRING_PHYS, COL_STRING_FUNC]\n",
    "    flag_cols = [COL_STICKY_FLAG, COL_EV_LFQ_FLAG, COL_EV_BG_FLAG, COL_NAME_MATCH]\n",
    "\n",
    "    all_dynamic_keyword_cols = set()\n",
    "\n",
    "    # --- Process Each Sheet ---\n",
    "    # Description: Iterates through each sheet found in the input Excel file.\n",
    "    processed_sheet_count = 0\n",
    "    total_sheets = len(sheet_names)\n",
    "\n",
    "    for i, sheet_name in enumerate(sheet_names):\n",
    "        logging.info(f\"--- Processing sheet {i+1}/{total_sheets}: '{sheet_name}' ---\")\n",
    "        df = None; df_sig = None # Initialize sheet DataFrames for this iteration.\n",
    "        try:\n",
    "            # --- Load Sheet Data ---\n",
    "            df = xlsx.parse(sheet_name)\n",
    "            logging.info(f\"Loaded sheet '{sheet_name}' ({df.shape[0]} rows x {df.shape[1]} columns)\")\n",
    "\n",
    "            # --- Check for Essential 'Gene names' Column ---\n",
    "            if COL_GENE_NAMES not in df.columns:\n",
    "                logging.warning(f\"Sheet '{sheet_name}' missing essential column '{COL_GENE_NAMES}'. Cannot process. Skipping annotation.\")\n",
    "                write_sheet_safely(writer, sheet_name, df)\n",
    "                processed_sheet_count += 1\n",
    "                continue\n",
    "\n",
    "            # --- Determine Rows to Annotate (Conditional Logic) ---\n",
    "            if COL_SIGNIFICANT in df.columns:\n",
    "                # Mode 1: 'Significant' column EXISTS. Filter based on its content.\n",
    "                logging.info(f\"Found '{COL_SIGNIFICANT}' column. Filtering for significant rows ('+').\")\n",
    "                try:\n",
    "                    df[COL_SIGNIFICANT] = df[COL_SIGNIFICANT].astype(str)\n",
    "                    df_sig = df[df[COL_SIGNIFICANT].str.strip() == \"+\"].copy()\n",
    "                    logging.info(f\"Found {df_sig.shape[0]} significant ('+') rows to process.\")\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error processing '{COL_SIGNIFICANT}' column in sheet '{sheet_name}': {e}. Skipping annotation for this sheet.\")\n",
    "                    write_sheet_safely(writer, sheet_name, df); processed_sheet_count += 1; continue\n",
    "            else:\n",
    "                # Mode 2: 'Significant' column is MISSING. Annotate all rows with valid gene names.\n",
    "                logging.info(f\"'{COL_SIGNIFICANT}' column not found in sheet '{sheet_name}'. Processing all rows with entries in '{COL_GENE_NAMES}'.\")\n",
    "                df_sig = df.dropna(subset=[COL_GENE_NAMES]).copy()\n",
    "                df_sig = df_sig[df_sig[COL_GENE_NAMES].astype(str).str.strip() != '']\n",
    "                logging.info(f\"Selected {df_sig.shape[0]} rows with gene names to process.\")\n",
    "\n",
    "            # --- Handle Empty Selection ---\n",
    "            if df_sig.empty:\n",
    "                logging.info(f\"No rows selected for annotation in sheet '{sheet_name}'. Writing empty annotated sheet.\")\n",
    "                empty_cols = list(df.columns)\n",
    "                # <<< MODIFIED: uniprot_go_cols is empty, so effectively removed >>>\n",
    "                generated_cols_order = (\n",
    "                     [COL_MAIN_GENE] + uniprot_basic_cols + uniprot_extra_static_cols +\n",
    "                     uniprot_go_cols + quickgo_cols +\n",
    "                     string_cols + gprofiler_cols + flag_cols\n",
    "                )\n",
    "                for col in generated_cols_order:\n",
    "                    if col not in empty_cols: empty_cols.append(col)\n",
    "                empty_df_sig = pd.DataFrame(columns=empty_cols)\n",
    "                write_sheet_safely(writer, sheet_name, empty_df_sig)\n",
    "                processed_sheet_count += 1\n",
    "                continue\n",
    "\n",
    "            # --- Extract Main Gene Symbol ---\n",
    "            try:\n",
    "                df_sig[COL_MAIN_GENE] = df_sig[COL_GENE_NAMES].apply(extract_main_gene_symbol)\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error extracting main gene symbol in sheet '{sheet_name}': {e}. '{COL_MAIN_GENE}' column will have NAs.\")\n",
    "                df_sig[COL_MAIN_GENE] = pd.NA\n",
    "\n",
    "            # --- Identify Unique Genes for Annotation ---\n",
    "            unique_genes = df_sig[COL_MAIN_GENE].dropna().unique().tolist()\n",
    "            logging.info(f\"Annotating {len(unique_genes)} unique valid gene symbols for '{sheet_name}'...\")\n",
    "\n",
    "            # --- Fetch Annotations Loop ---\n",
    "            sheet_dynamic_keyword_cols = set()\n",
    "            if not unique_genes:\n",
    "                logging.info(\"No valid unique gene symbols found to annotate.\")\n",
    "            else:\n",
    "                for gene in tqdm(unique_genes, desc=f\"Annotating '{sheet_name}'\", unit=\"gene\", ncols=100, leave=False):\n",
    "                    if not gene: continue\n",
    "\n",
    "                    # Fetch/Process UniProt data (excluding UniProt GO)\n",
    "                    uniprot_entry = fetch_uniprot_entry(gene)\n",
    "                    process_uniprot_basic_fields(gene, uniprot_entry)\n",
    "                    extra_result = process_uniprot_extra_fields(gene, uniprot_entry) # <<< No longer gets UniProt GO >>>\n",
    "                    if extra_result:\n",
    "                        for key in extra_result.keys():\n",
    "                            if isinstance(key, str) and key.startswith(\"UniProt KW:\"):\n",
    "                                sheet_dynamic_keyword_cols.add(key)\n",
    "\n",
    "                    # Fetch g:Profiler terms\n",
    "                    fetch_gprofiler_enrichment(gene)\n",
    "                    # time.sleep(...) removed\n",
    "\n",
    "                    # Fetch STRING interactors\n",
    "                    fetch_string_interactors(gene, network_type=\"physical\")\n",
    "                    fetch_string_interactors(gene, network_type=\"functional\")\n",
    "\n",
    "                    # Fetch QuickGO terms\n",
    "                    fetch_quickgo_annotations(gene)\n",
    "\n",
    "                logging.info(f\"Finished fetching annotations for '{sheet_name}'.\")\n",
    "                all_dynamic_keyword_cols.update(sheet_dynamic_keyword_cols)\n",
    "                if sheet_dynamic_keyword_cols:\n",
    "                    logging.info(f\"Dynamic UniProt Keyword columns found for this sheet: {sorted(list(sheet_dynamic_keyword_cols))}\")\n",
    "\n",
    "            # --- Define FULL Annotation Column List for Mapping ---\n",
    "            current_dynamic_keyword_cols_sorted = sorted(list(sheet_dynamic_keyword_cols))\n",
    "            # <<< MODIFIED: uniprot_go_cols is empty, effectively removed >>>\n",
    "            all_annotation_cols_for_mapping = (\n",
    "                uniprot_basic_cols +\n",
    "                uniprot_extra_static_cols +\n",
    "                # uniprot_go_cols + # This list is empty\n",
    "                quickgo_cols +\n",
    "                current_dynamic_keyword_cols_sorted +\n",
    "                string_cols +\n",
    "                gprofiler_cols\n",
    "            )\n",
    "\n",
    "            # --- Map Annotations to DataFrame ---\n",
    "            logging.info(f\"Mapping {len(all_annotation_cols_for_mapping)} annotation columns back to the sheet...\")\n",
    "            for col in all_annotation_cols_for_mapping:\n",
    "                if col not in df_sig.columns: df_sig[col] = pd.NA\n",
    "                try:\n",
    "                    gene_map_col_upper = df_sig[COL_MAIN_GENE].str.upper()\n",
    "                    if col in uniprot_basic_cols:\n",
    "                        cache = global_cache[\"uniprot_basic\"]\n",
    "                        df_sig[col] = gene_map_col_upper.map(lambda g: cache.get(g, {}).get(col, pd.NA) if pd.notna(g) else pd.NA)\n",
    "                    # <<< MODIFIED: Removed explicit check for COL_UNIPROT_GO_CC >>>\n",
    "                    elif col in uniprot_extra_static_cols or (isinstance(col, str) and col.startswith(\"UniProt KW:\")):\n",
    "                        cache = global_cache[\"uniprot_extra\"]\n",
    "                        df_sig[col] = gene_map_col_upper.map(lambda g: cache.get(g, {}).get(col, pd.NA) if pd.notna(g) else pd.NA)\n",
    "                    elif col == COL_STRING_PHYS:\n",
    "                        cache = global_cache[\"string_physical\"]\n",
    "                        df_sig[col] = gene_map_col_upper.map(cache)\n",
    "                    elif col == COL_STRING_FUNC:\n",
    "                        cache = global_cache[\"string_functional\"]\n",
    "                        df_sig[col] = gene_map_col_upper.map(cache)\n",
    "                    elif col in gprofiler_cols:\n",
    "                        cache = global_cache[\"gprofiler\"]\n",
    "                        df_sig[col] = gene_map_col_upper.map(lambda g: cache.get(g, {}).get(col, pd.NA) if pd.notna(g) else pd.NA)\n",
    "                    elif col == \"QuickGO:CC Terms\":\n",
    "                        cache = global_cache[\"quickgo_cc\"]\n",
    "                        df_sig[col] = gene_map_col_upper.map(cache)\n",
    "                    elif col == \"QuickGO:MF Terms\":\n",
    "                        cache = global_cache[\"quickgo_mf\"]\n",
    "                        df_sig[col] = gene_map_col_upper.map(cache)\n",
    "                    elif col == \"QuickGO:BP Terms\":\n",
    "                        cache = global_cache[\"quickgo_bp\"]\n",
    "                        df_sig[col] = gene_map_col_upper.map(cache)\n",
    "                except Exception as map_e:\n",
    "                    logging.error(f\"Error mapping column '{col}' in sheet '{sheet_name}': {map_e}. Filling with NA.\")\n",
    "                    df_sig[col] = pd.NA\n",
    "\n",
    "            # --- Apply Flags and QC Checks ---\n",
    "            logging.info(\"Applying flags and QC checks...\")\n",
    "            if COL_PROTEIN_DESC in df_sig.columns:\n",
    "                df_sig[COL_STICKY_FLAG] = df_sig[COL_PROTEIN_DESC].apply(detect_sticky_binder)\n",
    "            else:\n",
    "                logging.warning(f\"Cannot apply Sticky Flag: Column '{COL_PROTEIN_DESC}' missing in sheet '{sheet_name}'.\")\n",
    "                df_sig[COL_STICKY_FLAG] = \"\"\n",
    "            if set(CONFIG[\"ev_columns\"]).issubset(df_sig.columns) and COL_LOG2FC in df_sig.columns:\n",
    "                df_sig[COL_EV_LFQ_FLAG] = df_sig.apply(detect_ev_lfq_background, axis=1)\n",
    "            else:\n",
    "                 logging.warning(f\"Skipping EV LFQ Background flag in sheet '{sheet_name}': Required columns missing (e.g., {CONFIG['ev_columns']}, {COL_LOG2FC}).\")\n",
    "                 df_sig[COL_EV_LFQ_FLAG] = \"\"\n",
    "            if ev_background_genes:\n",
    "                df_sig[COL_EV_BG_FLAG] = df_sig[COL_MAIN_GENE].apply(lambda g: \"EV Control Background\" if pd.notna(g) and g in ev_background_genes else \"\")\n",
    "                logging.info(\"Applied EV Control Background flag.\")\n",
    "            else:\n",
    "                df_sig[COL_EV_BG_FLAG] = \"\"\n",
    "            if COL_PROTEIN_NAMES in df_sig.columns and COL_PROTEIN_DESC in df_sig.columns:\n",
    "                 df_sig[COL_NAME_MATCH] = df_sig.apply(lambda r: \"\" if pd.isna(r[COL_PROTEIN_NAMES]) or pd.isna(r[COL_PROTEIN_DESC]) else (\"\" if str(r[COL_PROTEIN_NAMES]).strip().lower() in str(r[COL_PROTEIN_DESC]).lower() else f\"Mismatch: Input='{str(r[COL_PROTEIN_NAMES])[:50]}' vs UniProt='{str(r[COL_PROTEIN_DESC])[:50]}'\"), axis=1)\n",
    "            else:\n",
    "                 logging.warning(f\"Skipping Name Match QC in sheet '{sheet_name}': Columns '{COL_PROTEIN_NAMES}' or '{COL_PROTEIN_DESC}' missing.\")\n",
    "                 df_sig[COL_NAME_MATCH] = \"\"\n",
    "\n",
    "            # --- Reorder Columns for Final Output ---\n",
    "            # Description: Arranges the columns in the `df_sig` DataFrame into a logical order,\n",
    "            #              using the corrected logic to insert annotations after 'Gene names'.\n",
    "            logging.info(\"Reordering columns for final output...\")\n",
    "            try:\n",
    "                # <<< Using Corrected Reordering Logic >>>\n",
    "                # Define the desired order of the newly generated/added columns.\n",
    "                # <<< MODIFIED: uniprot_go_cols is empty, effectively removed >>>\n",
    "                ordered_generated_cols = (\n",
    "                    [COL_MAIN_GENE] + uniprot_basic_cols + uniprot_extra_static_cols +\n",
    "                    # uniprot_go_cols + # This list is empty\n",
    "                    quickgo_cols + current_dynamic_keyword_cols_sorted +\n",
    "                    string_cols + gprofiler_cols + flag_cols\n",
    "                )\n",
    "                # Filter this list to include only columns actually present in the current df_sig.\n",
    "                ordered_generated_cols_present = [c for c in ordered_generated_cols if c in df_sig.columns]\n",
    "\n",
    "                # Determine the split point in the *original* columns list (`df.columns`)\n",
    "                original_cols = df.columns.tolist()\n",
    "                cols_before_insertion = []\n",
    "                cols_after_insertion = []\n",
    "\n",
    "                if COL_GENE_NAMES in original_cols:\n",
    "                    try:\n",
    "                        insert_idx = original_cols.index(COL_GENE_NAMES)\n",
    "                        cols_before_insertion = original_cols[:insert_idx + 1]\n",
    "                        cols_after_insertion = original_cols[insert_idx + 1:]\n",
    "                        logging.debug(f\"Reordering: Found '{COL_GENE_NAMES}' at index {insert_idx}.\")\n",
    "                    except ValueError:\n",
    "                        logging.warning(f\"'{COL_GENE_NAMES}' not found in original cols index? Using all original columns first.\")\n",
    "                        cols_before_insertion = original_cols\n",
    "                else:\n",
    "                    logging.warning(f\"'{COL_GENE_NAMES}' not found in original columns. Placing generated columns after all originals.\")\n",
    "                    cols_before_insertion = original_cols\n",
    "\n",
    "                # Construct the final column order carefully.\n",
    "                final_col_order = []\n",
    "                seen_cols = set()\n",
    "\n",
    "                # 1. Add original columns up to the insertion point (if they exist in df_sig).\n",
    "                for col in cols_before_insertion:\n",
    "                    if col in df_sig.columns and col not in seen_cols:\n",
    "                        final_col_order.append(col)\n",
    "                        seen_cols.add(col)\n",
    "\n",
    "                # 2. Add the generated annotation columns (if they exist in df_sig and haven't been added).\n",
    "                for col in ordered_generated_cols_present:\n",
    "                    if col in df_sig.columns and col not in seen_cols:\n",
    "                        final_col_order.append(col)\n",
    "                        seen_cols.add(col)\n",
    "\n",
    "                # 3. Add the remaining original columns (after insertion point) (if they exist in df_sig and haven't been added).\n",
    "                for col in cols_after_insertion:\n",
    "                     if col in df_sig.columns and col not in seen_cols:\n",
    "                        final_col_order.append(col)\n",
    "                        seen_cols.add(col)\n",
    "\n",
    "                # 4. Safety Net: Add any columns from df_sig that were missed entirely.\n",
    "                for col in df_sig.columns:\n",
    "                     if col not in seen_cols:\n",
    "                         logging.warning(f\"Column '{col}' appended at end during reordering (was missed from logic).\")\n",
    "                         final_col_order.append(col)\n",
    "\n",
    "                # Apply the reordered list to the DataFrame.\n",
    "                df_sig = df_sig[final_col_order]\n",
    "                logging.info(f\"Successfully reordered columns. Final shape for sheet '{sheet_name}': ({df_sig.shape[0]}x{df_sig.shape[1]})\")\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Column reordering failed for sheet '{sheet_name}': {e}. Using default order before saving.\")\n",
    "\n",
    "\n",
    "            # --- Save Processed Sheet ---\n",
    "            write_sheet_safely(writer, sheet_name, df_sig)\n",
    "            processed_sheet_count += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            # --- Fatal Error Handling for a Specific Sheet ---\n",
    "            logging.exception(f\"FATAL ERROR processing sheet '{sheet_name}': {e}\")\n",
    "            logging.info(f\"Attempting to write original data (or empty sheet) for errored sheet '{sheet_name}'.\")\n",
    "            try:\n",
    "                if 'df' in locals() and df is not None:\n",
    "                    write_sheet_safely(writer, sheet_name, df)\n",
    "                    processed_sheet_count += 1\n",
    "                else:\n",
    "                    logging.error(f\"Original df not available for errored sheet '{sheet_name}'. Writing empty DataFrame.\")\n",
    "                    write_sheet_safely(writer, sheet_name, pd.DataFrame())\n",
    "                    processed_sheet_count += 1\n",
    "            except Exception as nested_e:\n",
    "                logging.error(f\"Could not write original or empty data for errored sheet '{sheet_name}': {nested_e}\")\n",
    "\n",
    "    # === 14. FINALIZE EXCEL FILE ===\n",
    "    # Description: Closes the ExcelWriter, which triggers the final save of the output file.\n",
    "    #              Logs summary statistics about the run.\n",
    "    try:\n",
    "        if writer and hasattr(writer, 'book') and writer.book is not None:\n",
    "             writer.close() # Closing the writer saves the Excel file.\n",
    "             logging.info(f\"--- Main Processing Finished ---\")\n",
    "             logging.info(f\"âœ… Annotation and saving complete.\")\n",
    "             logging.info(f\"   Processed/Written Sheets: {processed_sheet_count}/{total_sheets}\")\n",
    "             logging.info(f\"   Output saved at: {output_path}\")\n",
    "             logging.info(f\"   Log file saved at: {log_file_name}\")\n",
    "             if all_dynamic_keyword_cols:\n",
    "                 logging.info(f\"   Unique dynamic UniProt Keyword columns generated across all sheets: {sorted(list(all_dynamic_keyword_cols))}\")\n",
    "        elif writer:\n",
    "            logging.warning(\"Excel writer object existed but seemed invalid for final closing/saving. Output might be incomplete.\")\n",
    "        else:\n",
    "            logging.error(\"Excel writer object was not created successfully. No output file generated.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"CRITICAL: Failed to close or finalize the Excel writer: {e}\")\n",
    "\n",
    "    logging.info(f\"--- Script Execution Finished ---\")\n",
    "\n",
    "# === 15. SCRIPT EXECUTION POINT ===\n",
    "# Description: This block ensures that the `main` function is called only when the script\n",
    "#              is executed directly (not imported as a module). It also records and logs\n",
    "#              the total execution time. Includes commented-out code for clearing caches.\n",
    "if __name__ == \"__main__\":\n",
    "    start_time = time.time() # Record the start time.\n",
    "\n",
    "    # --- Optional: Clear Caches for Fresh Run ---\n",
    "    # Description: Useful in interactive environments (like Jupyter) to force refetching\n",
    "    #              of all API data on every run, ignoring previously cached results.\n",
    "    # global_cache = {\n",
    "    #  \"uniprot_entry\": {}, \"uniprot_basic\": {}, \"uniprot_extra\": {},\n",
    "    #  \"gprofiler\": {}, \"string_physical\": {}, \"string_functional\": {},\n",
    "    #  \"quickgo_cc\": {}, \"quickgo_mf\": {}, \"quickgo_bp\": {}\n",
    "    # }\n",
    "    # logging.info(\"Global caches cleared for new run.\")\n",
    "    # ---------------------------------------------\n",
    "\n",
    "    # Call the main processing function defined above.\n",
    "    main()\n",
    "\n",
    "    end_time = time.time() # Record the end time.\n",
    "    duration = end_time - start_time # Calculate the duration.\n",
    "    # Log the total time taken for the script execution.\n",
    "    logging.info(f\"Total script execution time: {duration:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e01851-efee-4dcb-ada2-d198a933509a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
